{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:45:29.032808: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-17 15:45:29.062107: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734421529.081771 1456072 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734421529.087751 1456072 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-17 15:45:29.109808: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Trains CNN model with optimal hyper-parameters.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers import Conv1D, Dense, Flatten, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import  LearningRateScheduler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "from pathlib import Path\n",
    "import click\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "functional_groups = {\n",
    "    'Acid anhydride': Chem.MolFromSmarts('[CX3](=[OX1])[OX2][CX3](=[OX1])'),\n",
    "    'Acyl halide': Chem.MolFromSmarts('[CX3](=[OX1])[F,Cl,Br,I]'),\n",
    "    'Alcohol': Chem.MolFromSmarts('[#6][OX2H]'),\n",
    "    'Aldehyde': Chem.MolFromSmarts('[CX3H1](=O)[#6,H]'),\n",
    "    'Alkane': Chem.MolFromSmarts('[CX4;H3,H2]'),\n",
    "    'Alkene': Chem.MolFromSmarts('[CX3]=[CX3]'),\n",
    "    'Alkyne': Chem.MolFromSmarts('[CX2]#[CX2]'),\n",
    "    'Amide': Chem.MolFromSmarts('[NX3][CX3](=[OX1])[#6]'),\n",
    "    'Amine': Chem.MolFromSmarts('[NX3;H2,H1,H0;!$(NC=O)]'),\n",
    "    'Arene': Chem.MolFromSmarts('[cX3]1[cX3][cX3][cX3][cX3][cX3]1'),\n",
    "    'Azo compound': Chem.MolFromSmarts('[#6][NX2]=[NX2][#6]'),\n",
    "    'Carbamate': Chem.MolFromSmarts('[NX3][CX3](=[OX1])[OX2H0]'),\n",
    "    'Carboxylic acid': Chem.MolFromSmarts('[CX3](=O)[OX2H]'),\n",
    "    'Enamine': Chem.MolFromSmarts('[NX3][CX3]=[CX3]'),\n",
    "    'Enol': Chem.MolFromSmarts('[OX2H][#6X3]=[#6]'),\n",
    "    'Ester': Chem.MolFromSmarts('[#6][CX3](=O)[OX2H0][#6]'),\n",
    "    'Ether': Chem.MolFromSmarts('[OD2]([#6])[#6]'),\n",
    "    'Haloalkane': Chem.MolFromSmarts('[#6][F,Cl,Br,I]'),\n",
    "    'Hydrazine': Chem.MolFromSmarts('[NX3][NX3]'),\n",
    "    'Hydrazone': Chem.MolFromSmarts('[NX3][NX2]=[#6]'),\n",
    "    'Imide': Chem.MolFromSmarts('[CX3](=[OX1])[NX3][CX3](=[OX1])'),\n",
    "    'Imine': Chem.MolFromSmarts('[$([CX3]([#6])[#6]),$([CX3H][#6])]=[$([NX2][#6]),$([NX2H])]'),\n",
    "    'Isocyanate': Chem.MolFromSmarts('[NX2]=[C]=[O]'),\n",
    "    'Isothiocyanate': Chem.MolFromSmarts('[NX2]=[C]=[S]'),\n",
    "    'Ketone': Chem.MolFromSmarts('[#6][CX3](=O)[#6]'),\n",
    "    'Nitrile': Chem.MolFromSmarts('[NX1]#[CX2]'),\n",
    "    'Phenol': Chem.MolFromSmarts('[OX2H][cX3]:[c]'),\n",
    "    'Phosphine': Chem.MolFromSmarts('[PX3]'),\n",
    "    'Sulfide': Chem.MolFromSmarts('[#16X2H0]'),\n",
    "    'Sulfonamide': Chem.MolFromSmarts('[#16X4]([NX3])(=[OX1])(=[OX1])[#6]'),\n",
    "    'Sulfonate': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[OX2H0]'),\n",
    "    'Sulfone': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[#6]'),\n",
    "    'Sulfonic acid': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[OX2H]'),\n",
    "    'Sulfoxide': Chem.MolFromSmarts('[#16X3]=[OX1]'),\n",
    "    'Thial': Chem.MolFromSmarts('[CX3H1](=S)[#6,H]'),\n",
    "    'Thioamide': Chem.MolFromSmarts('[NX3][CX3]=[SX1]'),\n",
    "    'Thiol': Chem.MolFromSmarts('[#16X2H]')\n",
    "}\n",
    "\n",
    "\n",
    "def match_group(mol: Chem.Mol, func_group) -> int:\n",
    "    if type(func_group) == Chem.Mol:\n",
    "        n = len(mol.GetSubstructMatches(func_group))\n",
    "    else:\n",
    "        n = func_group(mol)\n",
    "    return 0 if n == 0 else 1\n",
    "\n",
    "def get_functional_groups(smiles: str) -> dict:\n",
    "    RDLogger.DisableLog('rdApp.*')\n",
    "    smiles = smiles.strip().replace(' ', '')\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: \n",
    "        return None\n",
    "    func_groups = list()\n",
    "    for func_group_name, smarts in functional_groups.items():\n",
    "        func_groups.append(match_group(mol, smarts))\n",
    "\n",
    "    return func_groups\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_test, num_fgs, aug, num, weighted):\n",
    "    \"\"\"Trains final model with the best hyper-parameters.\"\"\"\n",
    "    # Input\n",
    "    print(X_train.shape)\n",
    "    X_train = X_train.reshape(X_train.shape[0], 600, 1)\n",
    "    print(X_train.shape)\n",
    "    # Shape of input data.\n",
    "    input_shape = X_train.shape[1:]\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    # 1st CNN layer.\n",
    "    x = Conv1D(filters=31,\n",
    "               kernel_size=(11), \n",
    "               strides=1,\n",
    "               padding='same')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    # 2nd CNN layer.\n",
    "    x = Conv1D(filters=62,\n",
    "       kernel_size=(11),\n",
    "       strides=1,\n",
    "       padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    # Flatten layer.\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # 1st dense layer.\n",
    "    x = Dense(4927, activation='relu')(x)\n",
    "    x = Dropout(0.48599073736368)(x)\n",
    "\n",
    "    # 2nd dense layer.\n",
    "    x = Dense(2785, activation='relu')(x)\n",
    "    x = Dropout(0.48599073736368)(x)\n",
    "\n",
    "    # 3rd dense layer.\n",
    "    x = Dense(1574, activation='relu')(x)\n",
    "    x = Dropout(0.48599073736368)(x)\n",
    "\n",
    "    output_tensor = Dense(num_fgs, activation='sigmoid')(x)\n",
    "    print('Model Construction')\n",
    "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "    model.summary()\n",
    "    optimizer = Adam()\n",
    "\n",
    "    if weighted == 1:\n",
    "\n",
    "        def calculate_class_weights(y_true):\n",
    "            number_dim = np.shape(y_true)[1]\n",
    "            weights = np.zeros((2, number_dim))\n",
    "            # Calculates weights for each label in a for loop.\n",
    "            for i in range(number_dim):\n",
    "                weights_n, weights_p = (y_train.shape[0]/(2 * (y_train[:,i] == 0).sum())), (y_train.shape[0]/(2 * (y_train[:,i] == 1).sum()))\n",
    "                # Weights could be log-dampened to avoid extreme weights for extremly unbalanced data.\n",
    "                weights[1, i], weights[0, i] = weights_p, weights_n\n",
    "\n",
    "            return weights.T\n",
    "\n",
    "        def get_weighted_loss(weights):\n",
    "            def weighted_loss(y_true, y_pred):\n",
    "                return K.mean((weights[:,0]**(1.0-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "            return weighted_loss\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss=get_weighted_loss(calculate_class_weights(y_train)))\n",
    "\n",
    "    else:\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "\n",
    "    def custom_learning_rate_schedular(epoch):\n",
    "        if epoch < 31:\n",
    "            return 2.5e-4\n",
    "        elif 31 <= epoch < 37:\n",
    "            return 2.5000001187436283e-05\n",
    "        elif 37 <= epoch < 42:\n",
    "            return 2.5000001187436284e-06\n",
    "\n",
    "    callback = [LearningRateScheduler(custom_learning_rate_schedular, verbose=1)]\n",
    "    print('Start training')\n",
    "    # Start training.\n",
    "    history = model.fit(x=X_train, y=y_train, epochs=41, batch_size=41, callbacks=callback)\n",
    "\n",
    "    prediction = model.predict(X_test)\n",
    "    return (prediction > 0.5).astype(int)\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Loaded Data:  0\n",
      "Loaded Data:  1\n",
      "Loaded Data:  2\n",
      "Loaded Data:  3\n",
      "Loaded Data:  4\n",
      "Loaded Data:  5\n",
      "Loaded Data:  6\n",
      "Loaded Data:  7\n",
      "Loaded Data:  8\n",
      "Loaded Data:  9\n",
      "Loaded Data:  10\n",
      "Loaded Data:  11\n",
      "Loaded Data:  12\n",
      "Loaded Data:  13\n",
      "Loaded Data:  14\n",
      "Loaded Data:  15\n",
      "Loaded Data:  16\n",
      "Loaded Data:  17\n",
      "Loaded Data:  18\n",
      "Loaded Data:  19\n",
      "Loaded Data:  20\n",
      "Loaded Data:  21\n",
      "Loaded Data:  22\n",
      "Loaded Data:  23\n",
      "Loaded Data:  24\n",
      "Loaded Data:  25\n",
      "Loaded Data:  26\n",
      "Loaded Data:  27\n",
      "Loaded Data:  28\n",
      "Loaded Data:  29\n",
      "Loaded Data:  30\n",
      "Loaded Data:  31\n",
      "Loaded Data:  32\n",
      "Loaded Data:  33\n",
      "Loaded Data:  34\n",
      "Loaded Data:  35\n",
      "Loaded Data:  36\n",
      "Loaded Data:  37\n",
      "Loaded Data:  38\n",
      "Loaded Data:  39\n",
      "Loaded Data:  40\n",
      "Loaded Data:  41\n",
      "Loaded Data:  42\n",
      "Loaded Data:  43\n",
      "Loaded Data:  44\n",
      "Loaded Data:  45\n",
      "Loaded Data:  46\n",
      "Loaded Data:  47\n",
      "Loaded Data:  48\n",
      "Loaded Data:  49\n",
      "Loaded Data:  50\n",
      "Loaded Data:  51\n",
      "Loaded Data:  52\n",
      "Loaded Data:  53\n",
      "Loaded Data:  54\n",
      "Loaded Data:  55\n",
      "Loaded Data:  56\n",
      "Loaded Data:  57\n",
      "Loaded Data:  58\n",
      "Loaded Data:  59\n",
      "Loaded Data:  60\n",
      "Loaded Data:  61\n",
      "Loaded Data:  62\n",
      "Loaded Data:  63\n",
      "Loaded Data:  64\n",
      "Loaded Data:  65\n",
      "Loaded Data:  66\n",
      "Loaded Data:  67\n",
      "Loaded Data:  68\n",
      "Loaded Data:  69\n",
      "Loaded Data:  70\n",
      "Loaded Data:  71\n",
      "Loaded Data:  72\n",
      "Loaded Data:  73\n",
      "Loaded Data:  74\n",
      "Loaded Data:  75\n",
      "Loaded Data:  76\n",
      "Loaded Data:  77\n",
      "Loaded Data:  78\n",
      "Loaded Data:  79\n",
      "Loaded Data:  80\n",
      "Loaded Data:  81\n",
      "Loaded Data:  82\n",
      "Loaded Data:  83\n",
      "Loaded Data:  84\n",
      "Loaded Data:  85\n",
      "Loaded Data:  86\n",
      "Loaded Data:  87\n",
      "Loaded Data:  88\n",
      "Loaded Data:  89\n",
      "Loaded Data:  90\n",
      "Loaded Data:  91\n",
      "Loaded Data:  92\n",
      "Loaded Data:  93\n",
      "Loaded Data:  94\n",
      "Loaded Data:  95\n",
      "Loaded Data:  96\n",
      "Loaded Data:  97\n",
      "Loaded Data:  98\n",
      "Loaded Data:  99\n",
      "Loaded Data:  100\n",
      "Loaded Data:  101\n",
      "Loaded Data:  102\n",
      "Loaded Data:  103\n",
      "Loaded Data:  104\n",
      "Loaded Data:  105\n",
      "Loaded Data:  106\n",
      "Loaded Data:  107\n",
      "Loaded Data:  108\n",
      "Loaded Data:  109\n",
      "Loaded Data:  110\n",
      "Loaded Data:  111\n",
      "Loaded Data:  112\n",
      "Loaded Data:  113\n",
      "Loaded Data:  114\n",
      "Loaded Data:  115\n",
      "Loaded Data:  116\n",
      "Loaded Data:  117\n",
      "Loaded Data:  118\n",
      "Loaded Data:  119\n",
      "Loaded Data:  120\n",
      "Loaded Data:  121\n",
      "Loaded Data:  122\n",
      "Loaded Data:  123\n",
      "Loaded Data:  124\n",
      "Loaded Data:  125\n",
      "Loaded Data:  126\n",
      "Loaded Data:  127\n",
      "Loaded Data:  128\n",
      "Loaded Data:  129\n",
      "Loaded Data:  130\n",
      "Loaded Data:  131\n",
      "Loaded Data:  132\n",
      "Loaded Data:  133\n",
      "Loaded Data:  134\n",
      "Loaded Data:  135\n",
      "Loaded Data:  136\n",
      "Loaded Data:  137\n",
      "Loaded Data:  138\n",
      "Loaded Data:  139\n",
      "Loaded Data:  140\n",
      "Loaded Data:  141\n",
      "Loaded Data:  142\n",
      "Loaded Data:  143\n",
      "Loaded Data:  144\n",
      "Loaded Data:  145\n",
      "Loaded Data:  146\n",
      "Loaded Data:  147\n",
      "Loaded Data:  148\n",
      "Loaded Data:  149\n",
      "Loaded Data:  150\n",
      "Loaded Data:  151\n",
      "Loaded Data:  152\n",
      "Loaded Data:  153\n",
      "Loaded Data:  154\n",
      "Loaded Data:  155\n",
      "Loaded Data:  156\n",
      "Loaded Data:  157\n",
      "Loaded Data:  158\n",
      "Loaded Data:  159\n",
      "Loaded Data:  160\n",
      "Loaded Data:  161\n",
      "Loaded Data:  162\n",
      "Loaded Data:  163\n",
      "Loaded Data:  164\n",
      "Loaded Data:  165\n",
      "Loaded Data:  166\n",
      "Loaded Data:  167\n",
      "Loaded Data:  168\n",
      "Loaded Data:  169\n",
      "Loaded Data:  170\n",
      "Loaded Data:  171\n",
      "Loaded Data:  172\n",
      "Loaded Data:  173\n",
      "Loaded Data:  174\n",
      "Loaded Data:  175\n",
      "Loaded Data:  176\n",
      "Loaded Data:  177\n",
      "Loaded Data:  178\n",
      "Loaded Data:  179\n",
      "Loaded Data:  180\n",
      "Loaded Data:  181\n",
      "Loaded Data:  182\n",
      "Loaded Data:  183\n",
      "Loaded Data:  184\n",
      "Loaded Data:  185\n",
      "Loaded Data:  186\n",
      "Loaded Data:  187\n",
      "Loaded Data:  188\n",
      "Loaded Data:  189\n",
      "Loaded Data:  190\n",
      "Loaded Data:  191\n",
      "Loaded Data:  192\n",
      "Loaded Data:  193\n",
      "Loaded Data:  194\n",
      "Loaded Data:  195\n",
      "Loaded Data:  196\n",
      "Loaded Data:  197\n",
      "Loaded Data:  198\n",
      "Loaded Data:  199\n",
      "Loaded Data:  200\n",
      "Loaded Data:  201\n",
      "Loaded Data:  202\n",
      "Loaded Data:  203\n",
      "Loaded Data:  204\n",
      "Loaded Data:  205\n",
      "Loaded Data:  206\n",
      "Loaded Data:  207\n",
      "Loaded Data:  208\n",
      "Loaded Data:  209\n",
      "Loaded Data:  210\n",
      "Loaded Data:  211\n",
      "Loaded Data:  212\n",
      "Loaded Data:  213\n",
      "Loaded Data:  214\n",
      "Loaded Data:  215\n",
      "Loaded Data:  216\n",
      "Loaded Data:  217\n",
      "Loaded Data:  218\n",
      "Loaded Data:  219\n",
      "Loaded Data:  220\n",
      "Loaded Data:  221\n",
      "Loaded Data:  222\n",
      "Loaded Data:  223\n",
      "Loaded Data:  224\n",
      "Loaded Data:  225\n",
      "Loaded Data:  226\n",
      "Loaded Data:  227\n",
      "Loaded Data:  228\n",
      "Loaded Data:  229\n",
      "Loaded Data:  230\n",
      "Loaded Data:  231\n",
      "Loaded Data:  232\n",
      "Loaded Data:  233\n",
      "Loaded Data:  234\n",
      "Loaded Data:  235\n",
      "Loaded Data:  236\n",
      "Loaded Data:  237\n",
      "Loaded Data:  238\n",
      "Loaded Data:  239\n",
      "Loaded Data:  240\n",
      "Loaded Data:  241\n",
      "Loaded Data:  242\n",
      "Loaded Data:  243\n",
      "Loaded Data:  244\n"
     ]
    }
   ],
   "source": [
    "def interpolate_to_600(spec):\n",
    "    \n",
    "\n",
    "    old_x = np.arange(len(spec))\n",
    "    new_x = np.linspace(min(old_x), max(old_x), 600)\n",
    "\n",
    "    interp = interp1d(old_x, spec)\n",
    "    new_spec = interp(new_x)\n",
    "    return new_spec\n",
    "\n",
    "def make_msms_spectrum(spectrum):\n",
    "    msms_spectrum = np.zeros(10000)\n",
    "    for peak in spectrum:\n",
    "        peak_pos = int(peak[0]*10)\n",
    "        if peak_pos >= 10000:\n",
    "            peak_pos = 9999\n",
    "\n",
    "        msms_spectrum[peak_pos] = peak[1]\n",
    "    \n",
    "    return msms_spectrum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "analytical_data = Path(\"/data/zjh2/multimodal-spectroscopic-dataset-main/data/multimodal_spectroscopic_dataset\")\n",
    "out_path = Path(\"/home/dwj/icml_guangpu/multimodal-spectroscopic-dataset-main/runs/runs_f_groups/h_nmr\")\n",
    "column = \"h_nmr_spectra\"\n",
    "seed = 3245\n",
    "print(\"Loading Data\")\n",
    "training_data = None\n",
    "\n",
    "if column == 'pos_msms':\n",
    "    column = 'msms_positive_40ev'\n",
    "elif column == 'neg_msms':\n",
    "    column = 'msms_negative_40ev'\n",
    "\n",
    "for i, parquet_file in enumerate(analytical_data.glob(\"*.parquet\")):\n",
    "    data = pd.read_parquet(parquet_file, columns=[column, 'smiles'])\n",
    "\n",
    "    if 'msms' in column:\n",
    "        data[column] = data[column].map(make_msms_spectrum)\n",
    "\n",
    "    data['func_group'] = data.smiles.map(get_functional_groups)\n",
    "    data[column] = data[column].map(interpolate_to_600)\n",
    "\n",
    "    if training_data is None:\n",
    "        training_data = data\n",
    "    else:\n",
    "        training_data = pd.concat((training_data, data))\n",
    "    del data\n",
    "\n",
    "    print(\"Loaded Data: \", i)\n",
    "\n",
    "train, test = train_test_split(training_data, test_size=0.1, random_state=seed) \n",
    "\n",
    "X_train = np.stack(train[column].to_list())\n",
    "y_train = np.stack(train['func_group'].to_list())\n",
    "X_test = np.stack(test[column].to_list())\n",
    "y_test = np.stack(test['func_group'].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714962, 600)\n",
      "(714962, 600, 1)\n",
      "Model Construction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1734423152.992564 1456072 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">372</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">21,204</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">248</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9300</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4927</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">45,826,027</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4927</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2785</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">13,724,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2785</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1574</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,385,164</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1574</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">58,275</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m31\u001b[0m)        │           \u001b[38;5;34m372\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m31\u001b[0m)        │           \u001b[38;5;34m124\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m31\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m31\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m62\u001b[0m)        │        \u001b[38;5;34m21,204\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m62\u001b[0m)        │           \u001b[38;5;34m248\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m62\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m62\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9300\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4927\u001b[0m)           │    \u001b[38;5;34m45,826,027\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4927\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2785\u001b[0m)           │    \u001b[38;5;34m13,724,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2785\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1574\u001b[0m)           │     \u001b[38;5;34m4,385,164\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1574\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m)             │        \u001b[38;5;34m58,275\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,015,894</span> (244.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m64,015,894\u001b[0m (244.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,015,708</span> (244.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m64,015,708\u001b[0m (244.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">186</span> (744.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m186\u001b[0m (744.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 1/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3105s\u001b[0m 178ms/step - loss: 0.1719 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 2/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2869s\u001b[0m 165ms/step - loss: 0.1372 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 3/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2871s\u001b[0m 165ms/step - loss: 0.1205 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 4/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3307s\u001b[0m 190ms/step - loss: 0.1088 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 5/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3118s\u001b[0m 179ms/step - loss: 0.0991 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 6/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2979s\u001b[0m 171ms/step - loss: 0.0914 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 7/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3032s\u001b[0m 174ms/step - loss: 0.0848 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 8/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2866s\u001b[0m 164ms/step - loss: 0.0795 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 9/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3275s\u001b[0m 188ms/step - loss: 0.0749 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 10/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3866s\u001b[0m 222ms/step - loss: 0.0705 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 11/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4086s\u001b[0m 234ms/step - loss: 0.0672 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 12/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3928s\u001b[0m 225ms/step - loss: 0.0641 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 13/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4068s\u001b[0m 233ms/step - loss: 0.0618 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 14/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4179s\u001b[0m 240ms/step - loss: 0.0598 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 15/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4176s\u001b[0m 239ms/step - loss: 0.0571 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 16/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4218s\u001b[0m 242ms/step - loss: 0.0552 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 17/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4221s\u001b[0m 242ms/step - loss: 0.0535 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 18/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4221s\u001b[0m 242ms/step - loss: 0.0515 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 19/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4225s\u001b[0m 242ms/step - loss: 0.0499 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 20/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4226s\u001b[0m 242ms/step - loss: 0.0480 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 21/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4226s\u001b[0m 242ms/step - loss: 0.0462 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 22/41\n",
      "\u001b[1m17439/17439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4224s\u001b[0m 242ms/step - loss: 0.0451 - learning_rate: 2.5000e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 23/41\n",
      "\u001b[1m 8527/17439\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m36:19\u001b[0m 245ms/step - loss: 0.0433"
     ]
    }
   ],
   "source": [
    "# Train extended model.\n",
    "prediction = train_model(X_train, y_train, X_test, 37, 'e', 0, 0)\n",
    "\n",
    "print(f1_score(y_test, prediction, average='micro'))\n",
    "\n",
    "with open(out_path / \"results.pickle\", \"wb\") as file:\n",
    "    pickle.dump({'pred': prediction, 'tgt': y_test}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multispecdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
