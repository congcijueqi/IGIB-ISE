{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Disable RDLogger warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import os\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "functional_groups = {\n",
    "    'Acid anhydride': Chem.MolFromSmarts('[CX3](=[OX1])[OX2][CX3](=[OX1])'),\n",
    "    'Acyl halide': Chem.MolFromSmarts('[CX3](=[OX1])[F,Cl,Br,I]'),\n",
    "    'Alcohol': Chem.MolFromSmarts('[#6][OX2H]'),\n",
    "    'Aldehyde': Chem.MolFromSmarts('[CX3H1](=O)[#6,H]'),\n",
    "    'Alkane': Chem.MolFromSmarts('[CX4;H3,H2]'),\n",
    "    'Alkene': Chem.MolFromSmarts('[CX3]=[CX3]'),\n",
    "    'Alkyne': Chem.MolFromSmarts('[CX2]#[CX2]'),\n",
    "    'Amide': Chem.MolFromSmarts('[NX3][CX3](=[OX1])[#6]'),\n",
    "    'Amine': Chem.MolFromSmarts('[NX3;H2,H1,H0;!$(NC=O)]'),\n",
    "    'Arene': Chem.MolFromSmarts('[cX3]1[cX3][cX3][cX3][cX3][cX3]1'),\n",
    "    'Azo compound': Chem.MolFromSmarts('[#6][NX2]=[NX2][#6]'),\n",
    "    'Carbamate': Chem.MolFromSmarts('[NX3][CX3](=[OX1])[OX2H0]'),\n",
    "    'Carboxylic acid': Chem.MolFromSmarts('[CX3](=O)[OX2H]'),\n",
    "    'Enamine': Chem.MolFromSmarts('[NX3][CX3]=[CX3]'),\n",
    "    'Enol': Chem.MolFromSmarts('[OX2H][#6X3]=[#6]'),\n",
    "    'Ester': Chem.MolFromSmarts('[#6][CX3](=O)[OX2H0][#6]'),\n",
    "    'Ether': Chem.MolFromSmarts('[OD2]([#6])[#6]'),\n",
    "    'Haloalkane': Chem.MolFromSmarts('[#6][F,Cl,Br,I]'),\n",
    "    'Hydrazine': Chem.MolFromSmarts('[NX3][NX3]'),\n",
    "    'Hydrazone': Chem.MolFromSmarts('[NX3][NX2]=[#6]'),\n",
    "    'Imide': Chem.MolFromSmarts('[CX3](=[OX1])[NX3][CX3](=[OX1])'),\n",
    "    'Imine': Chem.MolFromSmarts('[$([CX3]([#6])[#6]),$([CX3H][#6])]=[$([NX2][#6]),$([NX2H])]'),\n",
    "    'Isocyanate': Chem.MolFromSmarts('[NX2]=[C]=[O]'),\n",
    "    'Isothiocyanate': Chem.MolFromSmarts('[NX2]=[C]=[S]'),\n",
    "    'Ketone': Chem.MolFromSmarts('[#6][CX3](=O)[#6]'),\n",
    "    'Nitrile': Chem.MolFromSmarts('[NX1]#[CX2]'),\n",
    "    'Phenol': Chem.MolFromSmarts('[OX2H][cX3]:[c]'),\n",
    "    'Phosphine': Chem.MolFromSmarts('[PX3]'),\n",
    "    'Sulfide': Chem.MolFromSmarts('[#16X2H0]'),\n",
    "    'Sulfonamide': Chem.MolFromSmarts('[#16X4]([NX3])(=[OX1])(=[OX1])[#6]'),\n",
    "    'Sulfonate': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[OX2H0]'),\n",
    "    'Sulfone': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[#6]'),\n",
    "    'Sulfonic acid': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[OX2H]'),\n",
    "    'Sulfoxide': Chem.MolFromSmarts('[#16X3]=[OX1]'),\n",
    "    'Thial': Chem.MolFromSmarts('[CX3H1](=S)[#6,H]'),\n",
    "    'Thioamide': Chem.MolFromSmarts('[NX3][CX3]=[SX1]'),\n",
    "    'Thiol': Chem.MolFromSmarts('[#16X2H]')\n",
    "}\n",
    "def match_group(mol: Chem.Mol, func_group) -> int:\n",
    "    if type(func_group) == Chem.Mol:\n",
    "        n = len(mol.GetSubstructMatches(func_group))\n",
    "    else:\n",
    "        n = func_group(mol)\n",
    "    return 0 if n == 0 else 1\n",
    "# Function to map SMILES to functional groups (no change)\n",
    "def get_functional_groups(smiles: str) -> dict:\n",
    "    smiles = smiles.strip().replace(' ', '')\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: \n",
    "        return None\n",
    "    func_groups = [match_group(mol, smarts) for smarts in functional_groups.values()]\n",
    "    return func_groups\n",
    "\n",
    "def interpolate_to_600(spec):\n",
    "    old_x = np.arange(len(spec))\n",
    "    new_x = np.linspace(min(old_x), max(old_x), 600)\n",
    "    interp = interp1d(old_x, spec)\n",
    "    return interp(new_x)\n",
    "\n",
    "def make_msms_spectrum(spectrum):\n",
    "    msms_spectrum = np.zeros(10000)\n",
    "    for peak in spectrum:\n",
    "        peak_pos = int(peak[0]*10)\n",
    "        peak_pos = min(peak_pos, 9999)\n",
    "        msms_spectrum[peak_pos] = peak[1]\n",
    "    return msms_spectrum\n",
    "\n",
    "# Define CNN Model in PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter_mean, scatter_std\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_fgs):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=31, kernel_size=11, padding='same')\n",
    "        self.conv2 = nn.Conv1d(in_channels=31, out_channels=62, kernel_size=11, padding='same')\n",
    "        self.fc1 = nn.Linear(62 * 150, 4927)\n",
    "        self.fc2 = nn.Linear(4927, 2785)\n",
    "        self.fc3 = nn.Linear(2785, 1574)\n",
    "        self.fc4 = nn.Linear(1574, num_fgs)\n",
    "        self.dropout = nn.Dropout(0.48599073736368)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(31)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(62)\n",
    "\n",
    "        # MLP for selecting important channels (62 channels)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(150, 128),  # Input 150 features per channel\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)     # Output importance score for each channel\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #torch.Size([41, 1, 600])\n",
    "        x = F.relu(self.batch_norm1(self.conv1(x)))\n",
    "        #torch.Size([41, 31, 600])\n",
    "        x = F.max_pool1d(x, 2)  #torch.Size([41, 31, 300])\n",
    "\n",
    "        x = F.relu(self.batch_norm2(self.conv2(x)))  #torch.Size([41, 62, 300])\n",
    "        x = F.max_pool1d(x, 2)  #torch.Size([41, 62, 150])\n",
    "\n",
    "        # 150维特征的通道重要性计算\n",
    "        # 对每个通道的150维特征进行平均\n",
    "        static_feature_map = x.clone().detach()\n",
    "        channel_means = x.mean(dim=1)  # torch.Size([41, 150])，每个通道的平均值\n",
    "        channel_std = x.std(dim=1)\n",
    "\n",
    "\n",
    "        # 使用MLP来预测每个通道的权重\n",
    "        channel_importance = torch.sigmoid(self.mlp(x))  # torch.Size([41, 62,1])\n",
    "        # 按照计算出的权重调整通道\n",
    "        ib_x_mean = x * channel_importance+(1-channel_importance)*channel_means.unsqueeze(1) \n",
    "        ib_x_std = (1-channel_importance) * channel_std.unsqueeze(1) \n",
    "        ib_x = ib_x_mean + torch.rand_like(ib_x_mean) * ib_x_std\n",
    "  # 通过广播机制，每个通道按权重调整\n",
    "\n",
    "        # 计算信息瓶颈损失：KL Divergence\n",
    "        epsilon = 1e-8  # 防止除零错误\n",
    "        KL_tensor_1 = 0.5 * ((ib_x_std ** 2) / (channel_std.unsqueeze(1)  + epsilon) ** 2 + (channel_std.unsqueeze(1)  ** 2) / (ib_x_std + epsilon) ** 2 - 1) + \\\n",
    "                   ((ib_x_mean - channel_means.unsqueeze(1) ) ** 2) / (channel_std.unsqueeze(1)  + epsilon) ** 2\n",
    "\n",
    "        KL_Loss_1KL_Loss_1 = torch.mean(KL_tensor_1)\n",
    "\n",
    "        # Flatten and pass through fully connected layers\n",
    "        x = x.view(ib_x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "\n",
    "        return x, KL_Loss_1KL_Loss_1\n",
    "        #return x\n",
    "\n",
    "# Example of how to use the model\n",
    "# model = CNNModel(num_fgs=10)\n",
    "# output, loss = model(torch.randn(41, 1, 600))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # 引入 tqdm\n",
    "\n",
    "b=0.0001\n",
    "def train_model(X_train, y_train, X_test, num_fgs, weighted=False, batch_size=41, epochs=41):\n",
    "    device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CNNModel(num_fgs).to(device)\n",
    "    \n",
    "    # Define optimizer and loss\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    if weighted:\n",
    "        class_weights = calculate_class_weights(y_train)\n",
    "        criterion = WeightedBinaryCrossEntropyLoss(class_weights).to(device)\n",
    "    else:\n",
    "        criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Create tqdm progress bar for each epoch\n",
    "        with tqdm(train_loader, unit='batch', desc=f\"Epoch {epoch+1}/{epochs}\") as tepoch:\n",
    "            for inputs, targets in tepoch:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs,loss1 = model(inputs.unsqueeze(1))  # Add channel dimension\n",
    "                loss2 = criterion(outputs, targets)\n",
    "                loss = loss2+loss1*b\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Update the progress bar with loss information\n",
    "                tepoch.set_postfix(loss=running_loss / (tepoch.n + 1))\n",
    "        \n",
    "        # After every epoch, print the average loss\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs,loss2 = model(inputs.unsqueeze(1))\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions)\n",
    "    return (predictions > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Data:  0\n",
      "Loaded Data:  1\n",
      "Loaded Data:  2\n",
      "Loaded Data:  3\n",
      "Loaded Data:  4\n",
      "Loaded Data:  5\n",
      "Loaded Data:  6\n",
      "Loaded Data:  7\n",
      "Loaded Data:  8\n",
      "Loaded Data:  9\n",
      "Loaded Data:  10\n",
      "Loaded Data:  11\n",
      "Loaded Data:  12\n",
      "Loaded Data:  13\n",
      "Loaded Data:  14\n",
      "Loaded Data:  15\n",
      "Loaded Data:  16\n",
      "Loaded Data:  17\n",
      "Loaded Data:  18\n",
      "Loaded Data:  19\n",
      "Loaded Data:  20\n",
      "Loaded Data:  21\n",
      "Loaded Data:  22\n",
      "Loaded Data:  23\n",
      "Loaded Data:  24\n",
      "Loaded Data:  25\n",
      "Loaded Data:  26\n",
      "Loaded Data:  27\n",
      "Loaded Data:  28\n",
      "Loaded Data:  29\n",
      "Loaded Data:  30\n",
      "Loaded Data:  31\n",
      "Loaded Data:  32\n",
      "Loaded Data:  33\n",
      "Loaded Data:  34\n",
      "Loaded Data:  35\n",
      "Loaded Data:  36\n",
      "Loaded Data:  37\n",
      "Loaded Data:  38\n",
      "Loaded Data:  39\n",
      "Loaded Data:  40\n",
      "Loaded Data:  41\n",
      "Loaded Data:  42\n",
      "Loaded Data:  43\n",
      "Loaded Data:  44\n",
      "Loaded Data:  45\n",
      "Loaded Data:  46\n",
      "Loaded Data:  47\n",
      "Loaded Data:  48\n",
      "Loaded Data:  49\n",
      "Loaded Data:  50\n",
      "Loaded Data:  51\n",
      "Loaded Data:  52\n",
      "Loaded Data:  53\n",
      "Loaded Data:  54\n",
      "Loaded Data:  55\n",
      "Loaded Data:  56\n",
      "Loaded Data:  57\n",
      "Loaded Data:  58\n",
      "Loaded Data:  59\n",
      "Loaded Data:  60\n",
      "Loaded Data:  61\n",
      "Loaded Data:  62\n",
      "Loaded Data:  63\n",
      "Loaded Data:  64\n",
      "Loaded Data:  65\n",
      "Loaded Data:  66\n",
      "Loaded Data:  67\n",
      "Loaded Data:  68\n",
      "Loaded Data:  69\n",
      "Loaded Data:  70\n",
      "Loaded Data:  71\n",
      "Loaded Data:  72\n",
      "Loaded Data:  73\n",
      "Loaded Data:  74\n",
      "Loaded Data:  75\n",
      "Loaded Data:  76\n",
      "Loaded Data:  77\n",
      "Loaded Data:  78\n",
      "Loaded Data:  79\n",
      "Loaded Data:  80\n",
      "Loaded Data:  81\n",
      "Loaded Data:  82\n",
      "Loaded Data:  83\n",
      "Loaded Data:  84\n",
      "Loaded Data:  85\n",
      "Loaded Data:  86\n",
      "Loaded Data:  87\n",
      "Loaded Data:  88\n",
      "Loaded Data:  89\n",
      "Loaded Data:  90\n",
      "Loaded Data:  91\n",
      "Loaded Data:  92\n",
      "Loaded Data:  93\n",
      "Loaded Data:  94\n",
      "Loaded Data:  95\n",
      "Loaded Data:  96\n",
      "Loaded Data:  97\n",
      "Loaded Data:  98\n",
      "Loaded Data:  99\n",
      "Loaded Data:  100\n",
      "Loaded Data:  101\n",
      "Loaded Data:  102\n",
      "Loaded Data:  103\n",
      "Loaded Data:  104\n",
      "Loaded Data:  105\n",
      "Loaded Data:  106\n",
      "Loaded Data:  107\n",
      "Loaded Data:  108\n",
      "Loaded Data:  109\n",
      "Loaded Data:  110\n",
      "Loaded Data:  111\n",
      "Loaded Data:  112\n",
      "Loaded Data:  113\n",
      "Loaded Data:  114\n",
      "Loaded Data:  115\n",
      "Loaded Data:  116\n",
      "Loaded Data:  117\n",
      "Loaded Data:  118\n",
      "Loaded Data:  119\n",
      "Loaded Data:  120\n",
      "Loaded Data:  121\n",
      "Loaded Data:  122\n",
      "Loaded Data:  123\n",
      "Loaded Data:  124\n",
      "Loaded Data:  125\n",
      "Loaded Data:  126\n",
      "Loaded Data:  127\n",
      "Loaded Data:  128\n",
      "Loaded Data:  129\n",
      "Loaded Data:  130\n",
      "Loaded Data:  131\n",
      "Loaded Data:  132\n",
      "Loaded Data:  133\n",
      "Loaded Data:  134\n",
      "Loaded Data:  135\n",
      "Loaded Data:  136\n",
      "Loaded Data:  137\n",
      "Loaded Data:  138\n",
      "Loaded Data:  139\n",
      "Loaded Data:  140\n",
      "Loaded Data:  141\n",
      "Loaded Data:  142\n",
      "Loaded Data:  143\n",
      "Loaded Data:  144\n",
      "Loaded Data:  145\n",
      "Loaded Data:  146\n",
      "Loaded Data:  147\n",
      "Loaded Data:  148\n",
      "Loaded Data:  149\n",
      "Loaded Data:  150\n",
      "Loaded Data:  151\n",
      "Loaded Data:  152\n",
      "Loaded Data:  153\n",
      "Loaded Data:  154\n",
      "Loaded Data:  155\n",
      "Loaded Data:  156\n",
      "Loaded Data:  157\n",
      "Loaded Data:  158\n",
      "Loaded Data:  159\n",
      "Loaded Data:  160\n",
      "Loaded Data:  161\n",
      "Loaded Data:  162\n",
      "Loaded Data:  163\n",
      "Loaded Data:  164\n",
      "Loaded Data:  165\n",
      "Loaded Data:  166\n",
      "Loaded Data:  167\n",
      "Loaded Data:  168\n",
      "Loaded Data:  169\n",
      "Loaded Data:  170\n",
      "Loaded Data:  171\n",
      "Loaded Data:  172\n",
      "Loaded Data:  173\n",
      "Loaded Data:  174\n",
      "Loaded Data:  175\n",
      "Loaded Data:  176\n",
      "Loaded Data:  177\n",
      "Loaded Data:  178\n",
      "Loaded Data:  179\n",
      "Loaded Data:  180\n",
      "Loaded Data:  181\n",
      "Loaded Data:  182\n",
      "Loaded Data:  183\n",
      "Loaded Data:  184\n",
      "Loaded Data:  185\n",
      "Loaded Data:  186\n",
      "Loaded Data:  187\n",
      "Loaded Data:  188\n",
      "Loaded Data:  189\n",
      "Loaded Data:  190\n",
      "Loaded Data:  191\n",
      "Loaded Data:  192\n",
      "Loaded Data:  193\n",
      "Loaded Data:  194\n",
      "Loaded Data:  195\n",
      "Loaded Data:  196\n",
      "Loaded Data:  197\n",
      "Loaded Data:  198\n",
      "Loaded Data:  199\n",
      "Loaded Data:  200\n",
      "Loaded Data:  201\n",
      "Loaded Data:  202\n",
      "Loaded Data:  203\n",
      "Loaded Data:  204\n",
      "Loaded Data:  205\n",
      "Loaded Data:  206\n",
      "Loaded Data:  207\n",
      "Loaded Data:  208\n",
      "Loaded Data:  209\n",
      "Loaded Data:  210\n",
      "Loaded Data:  211\n",
      "Loaded Data:  212\n",
      "Loaded Data:  213\n",
      "Loaded Data:  214\n",
      "Loaded Data:  215\n",
      "Loaded Data:  216\n",
      "Loaded Data:  217\n",
      "Loaded Data:  218\n",
      "Loaded Data:  219\n",
      "Loaded Data:  220\n",
      "Loaded Data:  221\n",
      "Loaded Data:  222\n",
      "Loaded Data:  223\n",
      "Loaded Data:  224\n",
      "Loaded Data:  225\n",
      "Loaded Data:  226\n",
      "Loaded Data:  227\n",
      "Loaded Data:  228\n",
      "Loaded Data:  229\n",
      "Loaded Data:  230\n",
      "Loaded Data:  231\n",
      "Loaded Data:  232\n",
      "Loaded Data:  233\n",
      "Loaded Data:  234\n",
      "Loaded Data:  235\n",
      "Loaded Data:  236\n",
      "Loaded Data:  237\n",
      "Loaded Data:  238\n",
      "Loaded Data:  239\n",
      "Loaded Data:  240\n",
      "Loaded Data:  241\n",
      "Loaded Data:  242\n",
      "Loaded Data:  243\n",
      "Loaded Data:  244\n"
     ]
    }
   ],
   "source": [
    "# Loading data (no change)\n",
    "analytical_data = Path(\"/data/zjh2/multimodal-spectroscopic-dataset-main/data/multimodal_spectroscopic_dataset\")\n",
    "out_path = Path(\"/home/dwj/icml_guangpu/multimodal-spectroscopic-dataset-main/runs/runs_f_groups/h_nmr\")\n",
    "column = \"h_nmr_spectra\"\n",
    "seed = 3245\n",
    "\n",
    "training_data = None\n",
    "for i, parquet_file in enumerate(analytical_data.glob(\"*.parquet\")):\n",
    "    data = pd.read_parquet(parquet_file, columns=[column, 'smiles'])\n",
    "    data[column] = data[column].map(interpolate_to_600)\n",
    "    data['func_group'] = data.smiles.map(get_functional_groups)\n",
    "    print(\"Loaded Data: \", i)\n",
    "    if training_data is None:\n",
    "        training_data = data\n",
    "    else:\n",
    "        training_data = pd.concat((training_data, data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "\n",
    "X_train = np.stack(train[column].to_list())\n",
    "y_train = np.stack(train['func_group'].to_list())\n",
    "X_test = np.stack(test[column].to_list())\n",
    "y_test = np.stack(test['func_group'].to_list())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/41: 100%|██████████| 17439/17439 [14:47<00:00, 19.65batch/s, loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/41, Loss: 0.17175170208311824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/41: 100%|██████████| 17439/17439 [13:55<00:00, 20.88batch/s, loss=0.159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/41, Loss: 0.15904590773002442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/41: 100%|██████████| 17439/17439 [09:38<00:00, 30.16batch/s, loss=0.154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/41, Loss: 0.1536876848821445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/41: 100%|██████████| 17439/17439 [11:08<00:00, 26.07batch/s, loss=0.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/41, Loss: 0.1496898248826728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/41: 100%|██████████| 17439/17439 [10:22<00:00, 28.02batch/s, loss=0.147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/41, Loss: 0.1465038159850732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/41: 100%|██████████| 17439/17439 [10:03<00:00, 28.89batch/s, loss=0.144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/41, Loss: 0.14386171982759552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/41: 100%|██████████| 17439/17439 [09:33<00:00, 30.40batch/s, loss=0.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/41, Loss: 0.14159161970510661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/41: 100%|██████████| 17439/17439 [09:31<00:00, 30.50batch/s, loss=0.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/41, Loss: 0.13976322551445305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/41: 100%|██████████| 17439/17439 [09:34<00:00, 30.36batch/s, loss=0.138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/41, Loss: 0.13782890476519255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/41: 100%|██████████| 17439/17439 [09:59<00:00, 29.10batch/s, loss=0.136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/41, Loss: 0.1361409236132104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/41:  72%|███████▏  | 12542/17439 [06:46<02:19, 35.01batch/s, loss=0.134]"
     ]
    }
   ],
   "source": [
    "# Train extended model\n",
    "predictions = train_model(X_train, y_train, X_test, num_fgs=37, weighted=False)\n",
    "\n",
    "# Evaluate the model\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Save results\n",
    "with open(out_path / \"results.pickle\", \"wb\") as file:\n",
    "    pickle.dump({'pred': predictions, 'tgt': y_test}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
