{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Data from:  1\n",
      "Loaded Data from:  2\n",
      "X_train shape: (8740, 3, 1800)\n",
      "y_train shape: (8740,)\n",
      "X_test shape: (972, 3, 1800)\n",
      "y_test shape: (972,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/41:   0%|          | 0/214 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[41, 30, 20]' is invalid for input of size 73800",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 635\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_test shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# Train extended model\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_fgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m37\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweighted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    638\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39marray(item, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m y_test], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[0;32mIn[2], line 499\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(X_train, y_train, X_test, y_test, num_fgs, weighted, batch_size, epochs)\u001b[0m\n\u001b[1;32m    496\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    498\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 499\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add channel dimension\u001b[39;00m\n\u001b[1;32m    500\u001b[0m x_pred \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# [B, num_fgs]\u001b[39;00m\n\u001b[1;32m    501\u001b[0m kl_loss_cnn \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkl_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# 标量\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zs/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zs/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 390\u001b[0m, in \u001b[0;36mCNNModelWithVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    387\u001b[0m x1, x2, x3 \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m, :], x[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m, :], x[:, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m3\u001b[39m, :]  \u001b[38;5;66;03m# 每个 [B, 1, feature_dim]\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m# 分别通过三个独立的CNN\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m ib_x_1, kl1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, channel, feature_dim]\u001b[39;00m\n\u001b[1;32m    391\u001b[0m ib_x_2, kl2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn2(x2)\n\u001b[1;32m    392\u001b[0m ib_x_3, kl3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn3(x3)\n",
      "File \u001b[0;32m~/anaconda3/envs/zs/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zs/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 170\u001b[0m, in \u001b[0;36mIndependentCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# => (B, 1, 600)\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# 7) 进入全连接，把 600 reshape 成 (30, 20)，再映射到 150 维\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# => (B, 30, 20)\u001b[39;00m\n\u001b[1;32m    171\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)                 \u001b[38;5;66;03m# => (B, 30, 150)\u001b[39;00m\n\u001b[1;32m    173\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm1(x))       \u001b[38;5;66;03m# (B, 30, 150)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[41, 30, 20]' is invalid for input of size 73800"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Disable RDLogger warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import os\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "functional_groups = {\n",
    "    'Acid anhydride': Chem.MolFromSmarts('[CX3](=[OX1])[OX2][CX3](=[OX1])'),\n",
    "    'Acyl halide': Chem.MolFromSmarts('[CX3](=[OX1])[F,Cl,Br,I]'),\n",
    "    'Alcohol': Chem.MolFromSmarts('[#6][OX2H]'),\n",
    "    'Aldehyde': Chem.MolFromSmarts('[CX3H1](=O)[#6,H]'),\n",
    "    'Alkane': Chem.MolFromSmarts('[CX4;H3,H2]'),\n",
    "    'Alkene': Chem.MolFromSmarts('[CX3]=[CX3]'),\n",
    "    'Alkyne': Chem.MolFromSmarts('[CX2]#[CX2]'),\n",
    "    'Amide': Chem.MolFromSmarts('[NX3][CX3](=[OX1])[#6]'),\n",
    "    'Amine': Chem.MolFromSmarts('[NX3;H2,H1,H0;!$(NC=O)]'),\n",
    "    'Arene': Chem.MolFromSmarts('[cX3]1[cX3][cX3][cX3][cX3][cX3]1'),\n",
    "    'Azo compound': Chem.MolFromSmarts('[#6][NX2]=[NX2][#6]'),\n",
    "    'Carbamate': Chem.MolFromSmarts('[NX3][CX3](=[OX1])[OX2H0]'),\n",
    "    'Carboxylic acid': Chem.MolFromSmarts('[CX3](=O)[OX2H]'),\n",
    "    'Enamine': Chem.MolFromSmarts('[NX3][CX3]=[CX3]'),\n",
    "    'Enol': Chem.MolFromSmarts('[OX2H][#6X3]=[#6]'),\n",
    "    'Ester': Chem.MolFromSmarts('[#6][CX3](=O)[OX2H0][#6]'),\n",
    "    'Ether': Chem.MolFromSmarts('[OD2]([#6])[#6]'),\n",
    "    'Haloalkane': Chem.MolFromSmarts('[#6][F,Cl,Br,I]'),\n",
    "    'Hydrazine': Chem.MolFromSmarts('[NX3][NX3]'),\n",
    "    'Hydrazone': Chem.MolFromSmarts('[NX3][NX2]=[#6]'),\n",
    "    'Imide': Chem.MolFromSmarts('[CX3](=[OX1])[NX3][CX3](=[OX1])'),\n",
    "    'Imine': Chem.MolFromSmarts('[$([CX3]([#6])[#6]),$([CX3H][#6])]=[$([NX2][#6]),$([NX2H])]'),\n",
    "    'Isocyanate': Chem.MolFromSmarts('[NX2]=[C]=[O]'),\n",
    "    'Isothiocyanate': Chem.MolFromSmarts('[NX2]=[C]=[S]'),\n",
    "    'Ketone': Chem.MolFromSmarts('[#6][CX3](=O)[#6]'),\n",
    "    'Nitrile': Chem.MolFromSmarts('[NX1]#[CX2]'),\n",
    "    'Phenol': Chem.MolFromSmarts('[OX2H][cX3]:[c]'),\n",
    "    'Phosphine': Chem.MolFromSmarts('[PX3]'),\n",
    "    'Sulfide': Chem.MolFromSmarts('[#16X2H0]'),\n",
    "    'Sulfonamide': Chem.MolFromSmarts('[#16X4]([NX3])(=[OX1])(=[OX1])[#6]'),\n",
    "    'Sulfonate': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[OX2H0]'),\n",
    "    'Sulfone': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[#6]'),\n",
    "    'Sulfonic acid': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[OX2H]'),\n",
    "    'Sulfoxide': Chem.MolFromSmarts('[#16X3]=[OX1]'),\n",
    "    'Thial': Chem.MolFromSmarts('[CX3H1](=S)[#6,H]'),\n",
    "    'Thioamide': Chem.MolFromSmarts('[NX3][CX3]=[SX1]'),\n",
    "    'Thiol': Chem.MolFromSmarts('[#16X2H]')\n",
    "}\n",
    "def match_group(mol: Chem.Mol, func_group) -> int:\n",
    "    if type(func_group) == Chem.Mol:\n",
    "        n = len(mol.GetSubstructMatches(func_group))\n",
    "    else:\n",
    "        n = func_group(mol)\n",
    "    return 0 if n == 0 else 1\n",
    "# Function to map SMILES to functional groups (no change)\n",
    "def get_functional_groups(smiles: str) -> dict:\n",
    "    smiles = smiles.strip().replace(' ', '')\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: \n",
    "        return None\n",
    "    func_groups = [match_group(mol, smarts) for smarts in functional_groups.values()]\n",
    "    return func_groups\n",
    "\n",
    "def interpolate_to_600(spec):\n",
    "    old_x = np.arange(len(spec))\n",
    "    new_x = np.linspace(min(old_x), max(old_x), 600)\n",
    "    interp = interp1d(old_x, spec)\n",
    "    return interp(new_x)\n",
    "\n",
    "def make_msms_spectrum(spectrum):\n",
    "    msms_spectrum = np.zeros(10000)\n",
    "    for peak in spectrum:\n",
    "        peak_pos = int(peak[0]*10)\n",
    "        peak_pos = min(peak_pos, 9999)\n",
    "        msms_spectrum[peak_pos] = peak[1]\n",
    "    return msms_spectrum\n",
    "\n",
    "# Define CNN Model in PyTo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IndependentCNN(nn.Module):\n",
    "    def __init__(self, num_fgs):\n",
    "        super(IndependentCNN, self).__init__()\n",
    "        self.kernel_size = 11\n",
    "        \n",
    "        # 如果你的 PyTorch 版本较低，不支持 padding='same'，请用下面这句\n",
    "        self.offset_conv = nn.Conv1d(\n",
    "            in_channels=1,\n",
    "            out_channels=self.kernel_size,  # 这里=11\n",
    "            kernel_size=self.kernel_size,   # 这里=11\n",
    "            padding=5                       # 手动 padding=5 即可保持长度不变\n",
    "        )\n",
    "        # 如果版本较新，支持 'same'，可以改成：\n",
    "        # self.offset_conv = nn.Conv1d(1, self.kernel_size, kernel_size=self.kernel_size, padding='same')\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(20, 150),  # 将 20 映射到 150\n",
    "        )\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(30)\n",
    "\n",
    "        # MLP for selecting important channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(150, 128),  # Input 150 features per channel\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)     # Output importance score for each channel\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 的形状: (B, 1, 600)\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) 生成 1D 偏移量: offset形状 = (B, kernel_size=11, L=600)\n",
    "        offset = self.offset_conv(x)\n",
    "        # 注意: 此时 offset.shape = (B, 11, 600)\n",
    "\n",
    "        # 2) 构造插值用的 grid（跟 offset 一样都是 (B, 11, 600) 才能相加）\n",
    "        B, C, L = x.size()  # B, 1, 600\n",
    "        grid = torch.arange(L, dtype=torch.float32, device=x.device).unsqueeze(0)  # (1, 600)\n",
    "        grid = grid.unsqueeze(1).expand(B, self.kernel_size, -1)                  # (B, 11, 600)\n",
    "\n",
    "        # 3) 相加得到新的采样位置 new_grid，形状依然是 (B, 11, 600)\n",
    "        new_grid = grid + offset\n",
    "\n",
    "        # 4) 做 grid_sample 需要将坐标归一化到[-1, 1]，然后再把维度变成 (B, out_H, out_W, 2)\n",
    "        #    这里 out_H=600, out_W=11\n",
    "        new_grid = new_grid / (L - 1) * 2 - 1            # => 归一化到[-1, 1]，shape仍是 (B, 11, 600)\n",
    "        new_grid = new_grid.permute(0, 2, 1).unsqueeze(-1)  # => (B, 600, 11, 1)\n",
    "        new_grid = torch.cat(\n",
    "            [new_grid, torch.zeros_like(new_grid)], \n",
    "            dim=-1\n",
    "        )  # => (B, 600, 11, 2)\n",
    "\n",
    "        # 5) 将 x 最后一维扩张成“宽度”维度，以便 2D 采样\n",
    "        #    现在 x 形状 = (B, 1, 600, 1)\n",
    "        x = x.unsqueeze(3)\n",
    "\n",
    "        # 使用 grid_sample 做双线性插值\n",
    "        # 期望输出形状 = (B, 1, out_H=600, out_W=11)\n",
    "        x = F.grid_sample(\n",
    "            x, \n",
    "            new_grid, \n",
    "            mode='bilinear', \n",
    "            align_corners=True\n",
    "        )\n",
    "        # x 的形状 => (B, 1, 600, 11)\n",
    "\n",
    "        # 6) 在 kernel_size 这个维度(最后一维=11)上做平均 => (B, 1, 600)\n",
    "        x = x.mean(dim=-1)  # => (B, 1, 600)\n",
    "\n",
    "        # 7) 进入全连接，把 600 reshape 成 (30, 20)，再映射到 150 维\n",
    "        x = x.view(B, 30, 20)          # => (B, 30, 20)\n",
    "        x = self.fc(x)                 # => (B, 30, 150)\n",
    "\n",
    "        x = F.relu(self.batch_norm1(x))       # (B, 30, 150)\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 假设“通道”是维度 1（大小=30），\n",
    "        # “时间/特征”是维度 2（大小=150）。\n",
    "        # 若想对每个通道在时间维上做均值/方差，就对 dim=2 做统计:\n",
    "        # -------------------------------------------------\n",
    "        channel_means = x.mean(dim=2)  # => (B, 30)\n",
    "        channel_std   = x.std(dim=2)   # => (B, 30)\n",
    "\n",
    "        # 8) 计算通道重要性: 让 MLP 处理每个“通道×时间”的向量(大小=150)\n",
    "        channel_importance = torch.sigmoid(self.mlp(x))  # => (B, 30, 1)\n",
    "\n",
    "        # 做信息瓶颈(IB)处理\n",
    "        #   - ib_x_mean = importance * x + (1 - importance) * 均值\n",
    "        #   - ib_x_std  = (1 - importance) * std\n",
    "        ib_x_mean = x * channel_importance + (1 - channel_importance) * channel_means.unsqueeze(-1)\n",
    "        ib_x_std = (1 - channel_importance) * channel_std.unsqueeze(-1)\n",
    "        # 这里 ib_x_mean/ib_x_std 的形状都是 (B, 30, 150)\n",
    "\n",
    "        # 加噪\n",
    "        ib_x = ib_x_mean + torch.rand_like(ib_x_mean) * ib_x_std  # => (B, 30, 150)\n",
    "\n",
    "        # 9) 计算 KL 散度: 参照高斯分布间的 KL\n",
    "        epsilon = 1e-8\n",
    "        KL_tensor = 0.5 * (\n",
    "            (ib_x_std**2) / (channel_std.unsqueeze(-1) + epsilon)**2 +\n",
    "            (channel_std.unsqueeze(-1)**2) / (ib_x_std + epsilon)**2 - 1\n",
    "        ) + ((ib_x_mean - channel_means.unsqueeze(-1))**2) / (channel_std.unsqueeze(-1) + epsilon)**2\n",
    "\n",
    "        KL_Loss = torch.mean(KL_tensor)\n",
    "\n",
    "        return ib_x, KL_Loss\n",
    "\n",
    "\n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_dim]\n",
    "        q = self.query(x)  # [batch_size, seq_len, input_dim]\n",
    "        k = self.key(x)    # [batch_size, seq_len, input_dim]\n",
    "        v = self.value(x)  # [batch_size, seq_len, input_dim]\n",
    "        # 计算注意力分数\n",
    "        attention_scores = torch.bmm(q, k.transpose(1, 2))  # [batch_size, seq_len, seq_len]\n",
    "        attention_probs = self.softmax(attention_scores)   # [batch_size, seq_len, seq_len]\n",
    "        # 加权求和\n",
    "        output = torch.bmm(attention_probs, v)  # [batch_size, seq_len, input_dim]\n",
    "        return output\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设 IndependentCNN 和 SelfAttentionLayer 已在其他部分定义\n",
    "# from your_module import IndependentCNN, SelfAttentionLayer\n",
    "\n",
    "# 2. 修改后的 CNNModel 集成 VAE\n",
    "# ========================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设 IndependentCNN 和 SelfAttentionLayer 已在其他部分定义\n",
    "# from your_module import IndependentCNN, SelfAttentionLayer\n",
    "\n",
    "# ========================\n",
    "# 1. 条件互信息估计器（用于估计条件互信息）\n",
    "# ========================\n",
    "class MutualInformationMinimizer(nn.Module):\n",
    "    def __init__(self, input_dim_z, input_dim_x1, input_dim_x2, input_dim_x3, hidden_dim=128):\n",
    "        super(MutualInformationMinimizer, self).__init__()\n",
    "        # 判别器网络，用于估计 D(x3, x1, x2, z)\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim_z + input_dim_x1 + input_dim_x2 + input_dim_x3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # 输出概率\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, x1, x2, x3):\n",
    "        \"\"\"\n",
    "        前向传播，计算 D(x3, x1, x2, z)\n",
    "        \"\"\"\n",
    "        input = torch.cat([z, x1, x2, x3], dim=1)  # [B, input_dim_z + input_dim_x1 + input_dim_x2 + input_dim_x3]\n",
    "        D = self.discriminator(input)  # [B, 1]\n",
    "        return D\n",
    "    \n",
    "    def compute_loss(self, z, x1, x2, x3):\n",
    "        \"\"\"\n",
    "        计算互信息最小化的损失函数\n",
    "        \"\"\"\n",
    "        # 正样本：来自联合分布 p(x3, x1, x2, z)\n",
    "        D_joint = self.forward(z, x1, x2, x3)  # [B, 1]\n",
    "        # 负样本：打乱 x3，来自边际分布 p(x3)p(x1, x2, z)\n",
    "        batch_size = z.size(0)\n",
    "        idx = torch.randperm(batch_size).to(z.device)\n",
    "        x3_shuffled = x3[idx]\n",
    "        D_marginal = self.forward(z, x1, x2, x3_shuffled)  # [B, 1]\n",
    "        \n",
    "        # 标签\n",
    "        ones = torch.ones_like(D_joint)\n",
    "        zeros = torch.zeros_like(D_marginal)\n",
    "        \n",
    "        # 损失函数：Binary Cross Entropy\n",
    "        loss_joint = nn.BCELoss()(D_joint, ones)\n",
    "        loss_marginal = nn.BCELoss()(D_marginal, zeros)\n",
    "        loss = loss_joint + loss_marginal\n",
    "        return loss\n",
    "\n",
    "    def mutual_information_loss(self, z, x1, x2, x3):\n",
    "        \"\"\"\n",
    "        计算互信息的估计值，用于监控\n",
    "        \"\"\"\n",
    "        # 正样本\n",
    "        D_joint = self.forward(z, x1, x2, x3)  # [B, 1]\n",
    "        # 负样本\n",
    "        batch_size = z.size(0)\n",
    "        idx = torch.randperm(batch_size).to(z.device)\n",
    "        x3_shuffled = x3[idx]\n",
    "        D_marginal = self.forward(z, x1, x2, x3_shuffled)  # [B, 1]\n",
    "        \n",
    "        # 互信息估计（基于 f-GAN）\n",
    "        # I(X; Y) ≈ E[log D(x,y)] + E[log(1 - D(x,y'))]\n",
    "        mi_estimate = torch.mean(torch.log(D_joint + 1e-10)) + torch.mean(torch.log(1 - D_marginal + 1e-10))\n",
    "        return mi_estimate\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 2. 修改后的 CNNModel 集成 VAE\n",
    "# ========================\n",
    "class CNNModelWithVAE(nn.Module): \n",
    "    def __init__(self, num_fgs, channel=30, feature_dim=150, hidden_dim=256, latent_dim=64, m_dim=10):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "        - num_fgs: 预测目标的维度\n",
    "        - channel: 每个光谱的通道数（不同频率段）\n",
    "        - feature_dim: 每个光谱的特征维度\n",
    "        - hidden_dim: 隐藏层维度\n",
    "        - latent_dim: 潜在变量 z 的维度\n",
    "        - m_dim: 预测目标的维度（如有需要）\n",
    "        \"\"\"\n",
    "        super(CNNModelWithVAE, self).__init__()\n",
    "        self.channel = channel\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # 创建三个独立的CNN模块\n",
    "        self.cnn1 = IndependentCNN(num_fgs)\n",
    "        self.cnn2 = IndependentCNN(num_fgs)\n",
    "        self.cnn3 = IndependentCNN(num_fgs)\n",
    "\n",
    "        # 自注意力层，用于信息交互（保持原有结构）\n",
    "        self.attention = SelfAttentionLayer(input_dim=150)\n",
    "\n",
    "        # VAE Encoder: 将三个光谱特征融合成潜在表示 z\n",
    "        # 将 [B, 3*channel, feature_dim] 展平为 [B, 3*channel*feature_dim]\n",
    "        self.fc_fusion = nn.Sequential(\n",
    "            nn.Linear(3 * channel * feature_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # VAE Decoder: 从潜在表示 z 重建三个光谱\n",
    "        self.decoder = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, channel * feature_dim),\n",
    "                nn.ReLU()\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "\n",
    "        # 计算 log q(x3 | z, x1, x2)def __init__(self, input_dim_z, input_dim_x1, input_dim_x2, input_dim_x3, hidden_dim=128):\n",
    "        # 条件互信息估计器\n",
    "        self.q_estimator = MutualInformationMinimizer(\n",
    "            input_dim_z=latent_dim,\n",
    "            input_dim_x1=channel * feature_dim,\n",
    "            input_dim_x2= channel * feature_dim,\n",
    "            input_dim_x3= channel * feature_dim,\n",
    "            hidden_dim=hidden_dim\n",
    "        )\n",
    "\n",
    "        # 全连接层用于最终预测\n",
    "        self.fc1 = nn.Linear(latent_dim, 4927)  # 使用 z 作为输入\n",
    "        self.fc2 = nn.Linear(4927, 2785)\n",
    "        self.fc3 = nn.Linear(2785, 1574)\n",
    "        self.fc4 = nn.Linear(1574, num_fgs)\n",
    "        self.dropout = nn.Dropout(0.48599073736368)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)   # ~ N(0, I)\n",
    "        return mu + std * eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数：\n",
    "        - x: 输入张量，形状为 [batch_size, 3, feature_dim]\n",
    "        \n",
    "        返回：\n",
    "        - 一个包含预测结果和各类损失组件的字典\n",
    "        \"\"\"\n",
    "        # 拆分输入为三个光谱通道\n",
    "        x1, x2, x3 = x[:, 0:1, :], x[:, 1:2, :], x[:, 2:3, :]  # 每个 [B, 1, feature_dim]\n",
    "\n",
    "        # 分别通过三个独立的CNN\n",
    "        ib_x_1, kl1 = self.cnn1(x1)  # [B, channel, feature_dim]\n",
    "        ib_x_2, kl2 = self.cnn2(x2)\n",
    "        ib_x_3, kl3 = self.cnn3(x3)\n",
    "\n",
    "        # 将三个通道的输出堆叠\n",
    "        ib_x_stacked = torch.cat([ib_x_1, ib_x_2, ib_x_3], dim=1)  # [B, 3*channel, feature_dim]\n",
    "        # 展平为 [B, 3*channel*feature_dim]\n",
    "        ib_x_flat = ib_x_stacked.view(ib_x_stacked.size(0), -1)  # [B, 3*channel*feature_dim]\n",
    "        # VAE Encoder\n",
    "        h = self.fc_fusion(ib_x_flat)  # [B, hidden_dim]\n",
    "        mu = self.fc_mu(h)             # [B, latent_dim]\n",
    "        logvar = self.fc_logvar(h)     # [B, latent_dim]\n",
    "        z = self.reparameterize(mu, logvar)  # [B, latent_dim]\n",
    "\n",
    "        # VAE Decoder: 重建三个光谱\n",
    "        recon_x = []\n",
    "        for decoder in self.decoder:\n",
    "            recon = decoder(z)  # [B, channel * feature_dim]\n",
    "            recon = recon.view(z.size(0), self.channel, self.feature_dim)  # [B, channel, feature_dim]\n",
    "            recon_x.append(recon)\n",
    "        recon_x1, recon_x2, recon_x3 = recon_x  # 各自的重构光谱\n",
    "\n",
    "        # 条件互信息估计器\n",
    "        # 将 ib_x_* 展平\n",
    "        ib_x1_flat = ib_x_1.view(z.size(0), -1)  # [B, channel * feature_dim]\n",
    "        ib_x2_flat = ib_x_2.view(z.size(0), -1)\n",
    "        ib_x3_flat = ib_x_3.view(z.size(0), -1)\n",
    "\n",
    "        log_q = self.q_estimator(z, ib_x1_flat, ib_x2_flat, ib_x3_flat).squeeze()  # [B]\n",
    "\n",
    "\n",
    "        # 条件互信息的上界损失\n",
    "        cmi_loss = (log_q).mean()\n",
    "        # 全连接层进行最终预测，使用 z 作为输入\n",
    "        x_pred = F.relu(self.fc1(z))  # [B, 4927]\n",
    "        x_pred = self.dropout(x_pred)\n",
    "        x_pred = F.relu(self.fc2(x_pred))  # [B, 2785]\n",
    "        x_pred = self.dropout(x_pred)\n",
    "        x_pred = F.relu(self.fc3(x_pred))  # [B, 1574]\n",
    "        x_pred = self.dropout(x_pred)\n",
    "        x_pred = torch.sigmoid(self.fc4(x_pred))  # [B, num_fgs]\n",
    "\n",
    "        # KL损失取平均值（来自三个CNN模块）\n",
    "        kl_loss = (kl1 + kl2 + kl3) / 3\n",
    "\n",
    "        return {\n",
    "            'x': x_pred,\n",
    "            'kl_loss': kl_loss,\n",
    "            'vae_mu': mu,\n",
    "            'vae_logvar': logvar,\n",
    "            'recon_x1': recon_x1,\n",
    "            'recon_x2': recon_x2,\n",
    "            'recon_x3': recon_x3,\n",
    "            'cmi_loss': cmi_loss,  # 注意这里是 cmi_loss\n",
    "            'ib_x_1': ib_x_1,\n",
    "            'ib_x_2': ib_x_2,\n",
    "            'ib_x_3': ib_x_3\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training function in PyTorch\n",
    "from tqdm import tqdm  # 引入 tqdm\n",
    "\n",
    "b=0.0001\n",
    "def train_model(X_train, y_train, X_test,y_test, num_fgs, weighted=False, batch_size=41, epochs=41):\n",
    "    device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CNNModelWithVAE(num_fgs).to(device)\n",
    "    \n",
    "    # Define optimizer and loss\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    if weighted:\n",
    "        class_weights = calculate_class_weights(y_train)\n",
    "        criterion = WeightedBinaryCrossEntropyLoss(class_weights).to(device)\n",
    "    else:\n",
    "        criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    # Create DataLoader\n",
    "    y_train = np.array([np.array(item, dtype=np.float32) for item in y_train], dtype=np.float32)\n",
    "    y_test = np.array([np.array(item, dtype=np.float32) for item in y_test], dtype=np.float32)\n",
    "    train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_kl_loss_cnn = 0.0\n",
    "        running_pred_loss = 0.0\n",
    "        running_kl_div = 0.0\n",
    "        running_cmi_loss = 0.0\n",
    "        running_recon_loss = 0.0\n",
    "        \n",
    "        # Create tqdm progress bar for each epoch\n",
    "        with tqdm(train_loader, unit='batch', desc=f\"Epoch {epoch+1}/{epochs}\") as tepoch:\n",
    "            for inputs, targets in tepoch:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)  # Add channel dimension\n",
    "                x_pred = outputs['x']  # [B, num_fgs]\n",
    "                kl_loss_cnn = outputs['kl_loss']  # 标量\n",
    "                mu = outputs['vae_mu']  # [B, latent_dim]\n",
    "                logvar = outputs['vae_logvar']  # [B, latent_dim]\n",
    "                recon_x1 = outputs['recon_x1']  # [B, input_dim]\n",
    "                recon_x2 = outputs['recon_x2']\n",
    "                recon_x3 = outputs['recon_x3']\n",
    "                cmi_loss = outputs['cmi_loss']  # 标量\n",
    "\n",
    "                # 预测损失（根据任务选择适当的损失函数，这里以MSE为例）\n",
    "                #print(\"------------------\")\n",
    "                #print(x_pred.size())#[41,37]\n",
    "                #print(targets.size())\n",
    "                pred_loss = criterion(x_pred, targets)\n",
    "\n",
    "                # VAE重构损失\n",
    "                recon_loss = F.mse_loss(recon_x1, outputs['ib_x_1']) + F.mse_loss(recon_x2, outputs['ib_x_2']) + F.mse_loss(recon_x3, outputs['ib_x_3'])\n",
    "                #recon_loss = 0\n",
    "                # VAE KL散度\n",
    "                kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1).mean()\n",
    "                lambda_recon = 0.1\n",
    "                lambda_kl = 0.1\n",
    "                lambda_cmi = 0.1\n",
    "                # 综合损失\n",
    "                #lambda_recon * recon_loss\n",
    "                #total_loss = kl_loss_cnn  + lambda_kl * kl_div + lambda_cmi * cmi_loss + pred_loss\n",
    "                total_loss = kl_loss_cnn  + pred_loss+ lambda_kl * kl_div + lambda_cmi * cmi_loss+ lambda_recon*recon_loss\n",
    "                total_loss.backward()\n",
    "                #pred_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += total_loss.item()\n",
    "                running_kl_loss_cnn += kl_loss_cnn.item()\n",
    "                running_pred_loss += pred_loss.item()\n",
    "                running_kl_div += kl_div.item()\n",
    "                running_cmi_loss += cmi_loss.item()\n",
    "                running_recon_loss += recon_loss.item()\n",
    "\n",
    "                # Update the progress bar with loss information\n",
    "                tepoch.set_postfix(loss=running_loss / (tepoch.n + 1))\n",
    "        \n",
    "        # After every epoch, print the average loss\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}')\n",
    "        print(f'Epoch {epoch+1}/{epochs}, running_kl_loss_cnn: {running_kl_loss_cnn / len(train_loader)}')\n",
    "        print(f'Epoch {epoch+1}/{epochs}, running_pred_loss: {running_pred_loss / len(train_loader)}')\n",
    "        print(f'Epoch {epoch+1}/{epochs}, running_kl_div: {running_kl_div / len(train_loader)}')\n",
    "        print(f'Epoch {epoch+1}/{epochs}, running_cmi_loss: {running_cmi_loss / len(train_loader)}')\n",
    "        print(f'Epoch {epoch+1}/{epochs}, running_recon_loss: {running_recon_loss / len(train_loader)}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)  # Add channel dimension\n",
    "            x_pred = outputs['x']  # [B, num_fgs]\n",
    "            predictions.append(x_pred.cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions)\n",
    "    return (predictions > 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Custom loss function with class weights\n",
    "class WeightedBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedBinaryCrossEntropyLoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = self.class_weights[0] * (1 - y_true) * torch.log(1 - y_pred + 1e-15) + \\\n",
    "               self.class_weights[1] * y_true * torch.log(y_pred + 1e-15)\n",
    "        return -loss.mean()\n",
    "\n",
    "# Calculate class weights\n",
    "def calculate_class_weights(y_true):\n",
    "    num_samples = y_true.shape[0]\n",
    "    class_weights = np.zeros((2, y_true.shape[1]))\n",
    "    for i in range(y_true.shape[1]):\n",
    "        weights_n = num_samples / (2 * (y_true[:, i] == 0).sum())\n",
    "        weights_p = num_samples / (2 * (y_true[:, i] == 1).sum())\n",
    "        class_weights[0, i] = weights_n\n",
    "        class_weights[1, i] = weights_p\n",
    "    return torch.tensor(class_weights.T, dtype=torch.float32)\n",
    "\n",
    "# Loading data (no change)\n",
    "analytical_data = Path(\"/data/zjh2/multimodal-spectroscopic-dataset-main/data/multimodal_spectroscopic_dataset\")\n",
    "out_path = Path(\"/home/dwj/icml_guangpu/multimodal-spectroscopic-dataset-main/runs/runs_f_groups/all\")\n",
    "columns = [\"h_nmr_spectra\", \"c_nmr_spectra\", \"ir_spectra\"]\n",
    "seed = 3245\n",
    "\n",
    "# 准备存储合并后的数据\n",
    "all_data = []\n",
    "i=0\n",
    "# 一次性读取文件并处理所有列\n",
    "for parquet_file in analytical_data.glob(\"*.parquet\"):\n",
    "    i+=1\n",
    "    # 读取所有需要的列\n",
    "    data = pd.read_parquet(parquet_file, columns=columns + ['smiles'])\n",
    "    \n",
    "    # 对每个列进行插值\n",
    "    for column in columns:\n",
    "        data[column] = data[column].map(interpolate_to_600)\n",
    "    \n",
    "    # 添加功能团信息\n",
    "    data['func_group'] = data.smiles.map(get_functional_groups)\n",
    "    all_data.append(data)\n",
    "    if i==3:\n",
    "        break\n",
    "    print(f\"Loaded Data from: \", i)\n",
    "    \n",
    "# 合并所有数据\n",
    "training_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "\n",
    "# 将数据划分为训练集和测试集\n",
    "train, test = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "\n",
    "# 定义特征列\n",
    "columns = [\"h_nmr_spectra\", \"c_nmr_spectra\", \"ir_spectra\"]\n",
    "\n",
    "# 提取训练集特征和标签\n",
    "X_train = np.array(train[columns].values.tolist())  # 确保特征值是一个二维数组\n",
    "y_train = np.array(train['func_group'].values)      # 标签转换为一维数组\n",
    "\n",
    "# 提取测试集特征和标签\n",
    "X_test = np.array(test[columns].values.tolist())    # 同样确保二维数组\n",
    "y_test = np.array(test['func_group'].values)        # 标签一维数组\n",
    "\n",
    "# 检查数组形状以验证正确性\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "# Train extended model\n",
    "predictions = train_model(X_train, y_train, X_test, y_test,num_fgs=37, weighted=False)\n",
    "\n",
    "# Evaluate the model\n",
    "y_test = np.array([np.array(item, dtype=np.float32) for item in y_test], dtype=np.float32)\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Save results\n",
    "with open(out_path / \"results.pickle\", \"wb\") as file:\n",
    "    pickle.dump({'pred': predictions, 'tgt': y_test}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
