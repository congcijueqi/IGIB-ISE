{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Data from:  1\n",
      "Loaded Data from:  2\n",
      "Loaded Data from:  3\n",
      "X_train shape: (8740, 3, 600)\n",
      "y_train shape: (8740,)\n",
      "X_test shape: (972, 3, 600)\n",
      "y_test shape: (972,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/41: 100%|██████████| 214/214 [00:06<00:00, 31.09batch/s, kl_weight=0.1, loss=0.453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/41, Loss: 0.4470481701264872, KL Weight: 0.1\n",
      "F1 Score: 0.683940418074853\n",
      "Best model saved with F1 Score: 0.683940418074853 at /home/dwj/icml_guangpu/multimodal-spectroscopic-dataset-main/runs/runs_f_groups/all/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/41: 100%|██████████| 214/214 [00:04<00:00, 43.36batch/s, kl_weight=0.2, loss=0.348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/41, Loss: 0.34285045993105273, KL Weight: 0.2\n",
      "F1 Score: 0.7316790352504638\n",
      "Best model saved with F1 Score: 0.7316790352504638 at /home/dwj/icml_guangpu/multimodal-spectroscopic-dataset-main/runs/runs_f_groups/all/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/41: 100%|██████████| 214/214 [00:04<00:00, 45.75batch/s, kl_weight=0.3, loss=0.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/41, Loss: 0.3229280066824405, KL Weight: 0.3\n",
      "F1 Score: 0.7141587301587301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/41: 100%|██████████| 214/214 [00:04<00:00, 43.98batch/s, kl_weight=0.4, loss=0.313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/41, Loss: 0.3087839241896834, KL Weight: 0.4\n",
      "F1 Score: 0.7314540059347181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/41: 100%|██████████| 214/214 [00:04<00:00, 46.71batch/s, kl_weight=0.5, loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/41, Loss: 0.2975108519753563, KL Weight: 0.5\n",
      "F1 Score: 0.7229611041405269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/41: 100%|██████████| 214/214 [00:05<00:00, 42.51batch/s, kl_weight=0.6, loss=0.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/41, Loss: 0.28412401021640993, KL Weight: 0.6\n",
      "F1 Score: 0.750599520383693\n",
      "Best model saved with F1 Score: 0.750599520383693 at /home/dwj/icml_guangpu/multimodal-spectroscopic-dataset-main/runs/runs_f_groups/all/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/41: 100%|██████████| 214/214 [00:05<00:00, 40.71batch/s, kl_weight=0.7, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/41, Loss: 0.27193780396586265, KL Weight: 0.7\n",
      "F1 Score: 0.7573824710607134\n",
      "Best model saved with F1 Score: 0.7573824710607134 at /home/dwj/icml_guangpu/multimodal-spectroscopic-dataset-main/runs/runs_f_groups/all/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/41: 100%|██████████| 214/214 [00:04<00:00, 43.30batch/s, kl_weight=0.8, loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/41, Loss: 0.25717710724500853, KL Weight: 0.8\n",
      "F1 Score: 0.7350918864858107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/41: 100%|██████████| 214/214 [00:04<00:00, 43.11batch/s, kl_weight=0.9, loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/41, Loss: 0.24151449274515438, KL Weight: 0.9\n",
      "F1 Score: 0.7648156789547363\n",
      "Best model saved with F1 Score: 0.7648156789547363 at /home/dwj/icml_guangpu/multimodal-spectroscopic-dataset-main/runs/runs_f_groups/all/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/41: 100%|██████████| 214/214 [00:05<00:00, 41.41batch/s, kl_weight=1, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/41, Loss: 0.22623701048927886, KL Weight: 1.0\n",
      "F1 Score: 0.7573529411764706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/41: 100%|██████████| 214/214 [00:05<00:00, 40.42batch/s, kl_weight=1, loss=0.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/41, Loss: 0.20954305950169252, KL Weight: 1.0\n",
      "F1 Score: 0.7476167778836987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/41: 100%|██████████| 214/214 [00:04<00:00, 42.85batch/s, kl_weight=1, loss=0.196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/41, Loss: 0.1929736864343982, KL Weight: 1.0\n",
      "F1 Score: 0.7585636664304276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/41: 100%|██████████| 214/214 [00:05<00:00, 41.97batch/s, kl_weight=1, loss=0.18] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/41, Loss: 0.17769180419289063, KL Weight: 1.0\n",
      "F1 Score: 0.7474105461393596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/41: 100%|██████████| 214/214 [00:04<00:00, 45.86batch/s, kl_weight=1, loss=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/41, Loss: 0.15929804832857347, KL Weight: 1.0\n",
      "F1 Score: 0.7483106105512745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/41: 100%|██████████| 214/214 [00:04<00:00, 43.15batch/s, kl_weight=1, loss=0.147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/41, Loss: 0.14445915142787952, KL Weight: 1.0\n",
      "F1 Score: 0.7515282272563826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/41: 100%|██████████| 214/214 [00:04<00:00, 44.42batch/s, kl_weight=1, loss=0.133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/41, Loss: 0.13103636963484444, KL Weight: 1.0\n",
      "F1 Score: 0.763503132017492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/41: 100%|██████████| 214/214 [00:04<00:00, 46.22batch/s, kl_weight=1, loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/41, Loss: 0.11751728080143438, KL Weight: 1.0\n",
      "F1 Score: 0.7517030772844726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/41: 100%|██████████| 214/214 [00:04<00:00, 44.10batch/s, kl_weight=1, loss=0.105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/41, Loss: 0.10322360314916228, KL Weight: 1.0\n",
      "F1 Score: 0.7573177842565597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/41: 100%|██████████| 214/214 [00:04<00:00, 45.36batch/s, kl_weight=1, loss=0.0943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/41, Loss: 0.09292958363472859, KL Weight: 1.0\n",
      "F1 Score: 0.7496420047732697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/41: 100%|██████████| 214/214 [00:04<00:00, 45.01batch/s, kl_weight=1, loss=0.0853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/41, Loss: 0.08410493237462556, KL Weight: 1.0\n",
      "F1 Score: 0.7582882671055725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/41: 100%|██████████| 214/214 [00:04<00:00, 44.86batch/s, kl_weight=1, loss=0.0764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/41, Loss: 0.07530259031140915, KL Weight: 1.0\n",
      "F1 Score: 0.7518410286382232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/41: 100%|██████████| 214/214 [00:04<00:00, 44.71batch/s, kl_weight=1, loss=0.0699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/41, Loss: 0.06890121927467462, KL Weight: 1.0\n",
      "F1 Score: 0.7545189504373178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/41: 100%|██████████| 214/214 [00:04<00:00, 46.87batch/s, kl_weight=1, loss=0.0643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/41, Loss: 0.06343046097067472, KL Weight: 1.0\n",
      "F1 Score: 0.757247230732972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/41: 100%|██████████| 214/214 [00:04<00:00, 45.82batch/s, kl_weight=1, loss=0.0599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/41, Loss: 0.05908206794585023, KL Weight: 1.0\n",
      "F1 Score: 0.7663681823467845\n",
      "Best model saved with F1 Score: 0.7663681823467845 at /home/dwj/icml_guangpu/multimodal-spectroscopic-dataset-main/runs/runs_f_groups/all/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/41: 100%|██████████| 214/214 [00:04<00:00, 46.30batch/s, kl_weight=1, loss=0.0535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/41, Loss: 0.052747306944889445, KL Weight: 1.0\n",
      "F1 Score: 0.7541905989919119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/41: 100%|██████████| 214/214 [00:05<00:00, 41.87batch/s, kl_weight=1, loss=0.0499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/41, Loss: 0.049441526400221286, KL Weight: 1.0\n",
      "F1 Score: 0.750464252553389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/41: 100%|██████████| 214/214 [00:05<00:00, 39.91batch/s, kl_weight=1, loss=0.0473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/41, Loss: 0.047049891551799866, KL Weight: 1.0\n",
      "F1 Score: 0.7524161794535258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/41: 100%|██████████| 214/214 [00:05<00:00, 40.36batch/s, kl_weight=1, loss=0.0447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/41, Loss: 0.0444848665393123, KL Weight: 1.0\n",
      "F1 Score: 0.7540600537445963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/41: 100%|██████████| 214/214 [00:05<00:00, 41.14batch/s, kl_weight=1, loss=0.0434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/41, Loss: 0.04254591232600892, KL Weight: 1.0\n",
      "F1 Score: 0.7560975609756098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/41: 100%|██████████| 214/214 [00:05<00:00, 40.51batch/s, kl_weight=1, loss=0.0408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/41, Loss: 0.04059110382554409, KL Weight: 1.0\n",
      "F1 Score: 0.7512057405011175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/41: 100%|██████████| 214/214 [00:05<00:00, 39.90batch/s, kl_weight=1, loss=0.0388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/41, Loss: 0.038391467244756, KL Weight: 1.0\n",
      "F1 Score: 0.7547169811320755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/41: 100%|██████████| 214/214 [00:05<00:00, 40.68batch/s, kl_weight=1, loss=0.0364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/41, Loss: 0.03644177988693814, KL Weight: 1.0\n",
      "F1 Score: 0.7637451115711985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/41: 100%|██████████| 214/214 [00:05<00:00, 42.17batch/s, kl_weight=1, loss=0.0357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/41, Loss: 0.03524770279998116, KL Weight: 1.0\n",
      "F1 Score: 0.7634508348794063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/41: 100%|██████████| 214/214 [00:04<00:00, 45.97batch/s, kl_weight=1, loss=0.035] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/41, Loss: 0.03448118334723132, KL Weight: 1.0\n",
      "F1 Score: 0.757010442332512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/41: 100%|██████████| 214/214 [00:04<00:00, 46.25batch/s, kl_weight=1, loss=0.0339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/41, Loss: 0.033408775986563936, KL Weight: 1.0\n",
      "F1 Score: 0.7526297127999054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/41: 100%|██████████| 214/214 [00:04<00:00, 46.40batch/s, kl_weight=1, loss=0.032] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/41, Loss: 0.031509255829362946, KL Weight: 1.0\n",
      "F1 Score: 0.7538116061931214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/41: 100%|██████████| 214/214 [00:04<00:00, 46.99batch/s, kl_weight=1, loss=0.0297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/41, Loss: 0.02926501562975556, KL Weight: 1.0\n",
      "F1 Score: 0.7537147537147537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/41: 100%|██████████| 214/214 [00:04<00:00, 47.43batch/s, kl_weight=1, loss=0.0297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/41, Loss: 0.029296606108894414, KL Weight: 1.0\n",
      "F1 Score: 0.7509505703422054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/41: 100%|██████████| 214/214 [00:04<00:00, 45.85batch/s, kl_weight=1, loss=0.0296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/41, Loss: 0.029202936021349976, KL Weight: 1.0\n",
      "F1 Score: 0.7600092571164082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/41: 100%|██████████| 214/214 [00:04<00:00, 47.02batch/s, kl_weight=1, loss=0.0301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/41, Loss: 0.02967795506816044, KL Weight: 1.0\n",
      "F1 Score: 0.7577712609970675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/41: 100%|██████████| 214/214 [00:04<00:00, 46.55batch/s, kl_weight=1, loss=0.0282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/41, Loss: 0.027810513699061682, KL Weight: 1.0\n",
      "F1 Score: 0.7527408444133427\n",
      "F1 Score: 0.7527408444133427\n"
     ]
    }
   ],
   "source": [
    "#论文中需要预测的部分，用 t3去预测y构建一个loss，  用t1，t2，t3预测y构建一个loss， t1和t2与 t3之间最小化构建一个loss\n",
    "#  t3与x3之间最小化构建一个loss， t1与x3，x1最小化构建一个loss，t2与x3和x2最小化构建一个loss\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Disable RDLogger warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import os\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "functional_groups = {\n",
    "    'Acid anhydride': Chem.MolFromSmarts('[CX3](=[OX1])[OX2][CX3](=[OX1])'),\n",
    "    'Acyl halide': Chem.MolFromSmarts('[CX3](=[OX1])[F,Cl,Br,I]'),\n",
    "    'Alcohol': Chem.MolFromSmarts('[#6][OX2H]'),\n",
    "    'Aldehyde': Chem.MolFromSmarts('[CX3H1](=O)[#6,H]'),\n",
    "    'Alkane': Chem.MolFromSmarts('[CX4;H3,H2]'),\n",
    "    'Alkene': Chem.MolFromSmarts('[CX3]=[CX3]'),\n",
    "    'Alkyne': Chem.MolFromSmarts('[CX2]#[CX2]'),\n",
    "    'Amide': Chem.MolFromSmarts('[NX3][CX3](=[OX1])[#6]'),\n",
    "    'Amine': Chem.MolFromSmarts('[NX3;H2,H1,H0;!$(NC=O)]'),\n",
    "    'Arene': Chem.MolFromSmarts('[cX3]1[cX3][cX3][cX3][cX3][cX3]1'),\n",
    "    'Azo compound': Chem.MolFromSmarts('[#6][NX2]=[NX2][#6]'),\n",
    "    'Carbamate': Chem.MolFromSmarts('[NX3][CX3](=[OX1])[OX2H0]'),\n",
    "    'Carboxylic acid': Chem.MolFromSmarts('[CX3](=O)[OX2H]'),\n",
    "    'Enamine': Chem.MolFromSmarts('[NX3][CX3]=[CX3]'),\n",
    "    'Enol': Chem.MolFromSmarts('[OX2H][#6X3]=[#6]'),\n",
    "    'Ester': Chem.MolFromSmarts('[#6][CX3](=O)[OX2H0][#6]'),\n",
    "    'Ether': Chem.MolFromSmarts('[OD2]([#6])[#6]'),\n",
    "    'Haloalkane': Chem.MolFromSmarts('[#6][F,Cl,Br,I]'),\n",
    "    'Hydrazine': Chem.MolFromSmarts('[NX3][NX3]'),\n",
    "    'Hydrazone': Chem.MolFromSmarts('[NX3][NX2]=[#6]'),\n",
    "    'Imide': Chem.MolFromSmarts('[CX3](=[OX1])[NX3][CX3](=[OX1])'),\n",
    "    'Imine': Chem.MolFromSmarts('[$([CX3]([#6])[#6]),$([CX3H][#6])]=[$([NX2][#6]),$([NX2H])]'),\n",
    "    'Isocyanate': Chem.MolFromSmarts('[NX2]=[C]=[O]'),\n",
    "    'Isothiocyanate': Chem.MolFromSmarts('[NX2]=[C]=[S]'),\n",
    "    'Ketone': Chem.MolFromSmarts('[#6][CX3](=O)[#6]'),\n",
    "    'Nitrile': Chem.MolFromSmarts('[NX1]#[CX2]'),\n",
    "    'Phenol': Chem.MolFromSmarts('[OX2H][cX3]:[c]'),\n",
    "    'Phosphine': Chem.MolFromSmarts('[PX3]'),\n",
    "    'Sulfide': Chem.MolFromSmarts('[#16X2H0]'),\n",
    "    'Sulfonamide': Chem.MolFromSmarts('[#16X4]([NX3])(=[OX1])(=[OX1])[#6]'),\n",
    "    'Sulfonate': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[OX2H0]'),\n",
    "    'Sulfone': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[#6]'),\n",
    "    'Sulfonic acid': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[OX2H]'),\n",
    "    'Sulfoxide': Chem.MolFromSmarts('[#16X3]=[OX1]'),\n",
    "    'Thial': Chem.MolFromSmarts('[CX3H1](=S)[#6,H]'),\n",
    "    'Thioamide': Chem.MolFromSmarts('[NX3][CX3]=[SX1]'),\n",
    "    'Thiol': Chem.MolFromSmarts('[#16X2H]')\n",
    "}\n",
    "def match_group(mol: Chem.Mol, func_group) -> int:\n",
    "    if type(func_group) == Chem.Mol:\n",
    "        n = len(mol.GetSubstructMatches(func_group))\n",
    "    else:\n",
    "        n = func_group(mol)\n",
    "    return 0 if n == 0 else 1\n",
    "# Function to map SMILES to functional groups (no change)\n",
    "def get_functional_groups(smiles: str) -> dict:\n",
    "    smiles = smiles.strip().replace(' ', '')\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: \n",
    "        return None\n",
    "    func_groups = [match_group(mol, smarts) for smarts in functional_groups.values()]\n",
    "    return func_groups\n",
    "\n",
    "def interpolate_to_600(spec):\n",
    "    old_x = np.arange(len(spec))\n",
    "    new_x = np.linspace(min(old_x), max(old_x), 600)\n",
    "    interp = interp1d(old_x, spec)\n",
    "    return interp(new_x)\n",
    "\n",
    "def make_msms_spectrum(spectrum):\n",
    "    msms_spectrum = np.zeros(10000)\n",
    "    for peak in spectrum:\n",
    "        peak_pos = int(peak[0]*10)\n",
    "        peak_pos = min(peak_pos, 9999)\n",
    "        msms_spectrum[peak_pos] = peak[1]\n",
    "    return msms_spectrum\n",
    "\n",
    "# Define CNN Model in PyTo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#论文中需要预测的部分，用 t3去预测y构建一个loss，  用t1，t2，t3预测y构建一个loss， t1和t2与 t3之间最小化构建一个loss\n",
    "#  t3与x3之间最小化构建一个loss， t1与x3，x1最小化构建一个loss，t2与x3和x2最小化构建一个loss\n",
    "\n",
    "class IndependentCNN_main(nn.Module):\n",
    "    def __init__(self, num_fgs):\n",
    "        super(IndependentCNN_main, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=31, kernel_size=11, padding='same')\n",
    "        self.conv2 = nn.Conv1d(in_channels=31, out_channels=62, kernel_size=11, padding='same')\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(31)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(62)\n",
    "\n",
    "        # MLP for selecting important channels (62 channels)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(62, 128),  # 输入每个通道150个特征\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)     # 输出每个通道的重要性评分\n",
    "        )\n",
    "\n",
    "    def compress(self, solute_features):\n",
    "        p = self.mlp(solute_features)\n",
    "        device = solute_features.device\n",
    "        temperature = 1.0\n",
    "        bias = 0.0001  # 避免 bias 为 0 导致的问题\n",
    "        eps = (bias - (1 - bias)) * torch.rand(p.size()) + (1 - bias)\n",
    "        gate_inputs = torch.log(eps) - torch.log(1 - eps)\n",
    "        gate_inputs = gate_inputs.to(device)\n",
    "        gate_inputs = (gate_inputs + p) / temperature\n",
    "        gate_inputs = torch.sigmoid(gate_inputs).squeeze()\n",
    "        p = torch.sigmoid(p)\n",
    "        return gate_inputs, p\n",
    "\n",
    "    def forward(self, x1,x2,x3):\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "        # 压缩与门控，依次压缩 1 2 3\n",
    "        \n",
    "    \n",
    "\n",
    "        # **x1部分开始**其中t_m只由x_m决定\n",
    "        x3 = F.relu(self.batch_norm1(self.conv1(x3)))\n",
    "        x3 = F.max_pool1d(x3, 2)  # 池化大小为1，不改变尺寸\n",
    "        x3 = F.relu(self.batch_norm2(self.conv2(x3)))\n",
    "        x3 = F.max_pool1d(x3, 2)  # 池化大小为4，减少特征维度\n",
    "        x3 = x3.permute(0, 2, 1)  # 调整维度顺序\n",
    "        channel_std_3 = x3.std(dim=1)\n",
    "        channel_importance_3, p_3 = self.compress(x3)\n",
    "        channel_importance_3 = channel_importance_3.unsqueeze(-1)\n",
    "        \n",
    "        ib_x3_mean = x3 * channel_importance_3  # 去除 (1 - channel_importance) * channel_means.unsqueeze(1)\n",
    "        ib_x3_std = (1 - channel_importance_3) * channel_std_3.unsqueeze(1)\n",
    "        ib_x3 = ib_x3_mean + torch.rand_like(ib_x3_mean) * ib_x3_std\n",
    "        \n",
    "         # KL 散度损失计算,先算 ta tm 是ta与他的0 1做kl\n",
    "        epsilon = 1e-8\n",
    "        KL_tensor = 0.5 * (\n",
    "            (ib_x3_std**2) / (channel_std_3.unsqueeze(1) + epsilon)**2 +\n",
    "            (channel_std_3.unsqueeze(1)**2) / (ib_x3_std + epsilon)**2 - 1\n",
    "        ) + (ib_x3_mean**2) / (channel_std_3.unsqueeze(1) + epsilon)**2  # 修改了这里，将 (ib_x_mean - 0)**2 替换为 ib_x_mean**2\n",
    "\n",
    "        KL_Loss_3 = torch.mean(KL_tensor)\n",
    "        # **修改部分结束**\n",
    "        \n",
    "        # **x2部分开始**，t_a是由另外三个x_m.t_m,x_a构成的\n",
    "        x2 = F.relu(self.batch_norm1(self.conv1(x2)))\n",
    "        x2 = F.max_pool1d(x2, 2)  # 池化大小为1，不改变尺寸\n",
    "        x2 = F.relu(self.batch_norm2(self.conv2(x2)))\n",
    "\n",
    "        x2 = F.max_pool1d(x2, 2)  # 池化大小为4，减少特征维度\n",
    "        x2 = x2.permute(0, 2, 1)  # 调整维度顺序.\n",
    "        x_2 = x3+ ib_x3+x2\n",
    "        channel_std_2 = x_2.std(dim=1)\n",
    "        channel_importance_2, p_2 = self.compress(x_2)\n",
    "        channel_importance_2 = channel_importance_2.unsqueeze(-1)\n",
    "        ib_x2_mean = x_2 * channel_importance_2  # 去除 (1 - channel_importance) * channel_means.unsqueeze(1)\n",
    "        ib_x2_std = (1 - channel_importance_2) * channel_std_2.unsqueeze(1)\n",
    "        ib_x2 = ib_x2_mean + torch.rand_like(ib_x2_mean) * ib_x2_std\n",
    "        \n",
    "        epsilon = 1e-8\n",
    "        KL_tensor = 0.5 * (\n",
    "            (ib_x2_std**2) / (channel_std_2.unsqueeze(1) + epsilon)**2 +\n",
    "            (channel_std_2.unsqueeze(1)**2) / (ib_x2_std + epsilon)**2 - 1\n",
    "        ) + (ib_x2_mean**2) / (channel_std_2.unsqueeze(1) + epsilon)**2  # 修改了这里，将 (ib_x_mean - 0)**2 替换为 ib_x_mean**2\n",
    "\n",
    "        KL_Loss_2 = torch.mean(KL_tensor)\n",
    "        # **修改部分结束**\n",
    "        \n",
    "        # **x3部分开始**，t_a是由另外三个x_m.t_m,x_a构成的\n",
    "        x1 = F.relu(self.batch_norm1(self.conv1(x1)))\n",
    "        x1 = F.max_pool1d(x1, 2)  # 池化大小为1，不改变尺寸\n",
    "        x1 = F.relu(self.batch_norm2(self.conv2(x1)))\n",
    "\n",
    "        x1 = F.max_pool1d(x1, 2)  # 池化大小为4，减少特征维度\n",
    "        x1 = x1.permute(0, 2, 1)  # 调整维度顺序.\n",
    "        x_1 = x3+ ib_x3+x1\n",
    "        channel_std_1 = x_1.std(dim=1)\n",
    "        channel_importance_1, p_1 = self.compress(x_1)\n",
    "        channel_importance_1 = channel_importance_1.unsqueeze(-1)\n",
    "        ib_x1_mean = x_1 * channel_importance_1  # 去除 (1 - channel_importance) * channel_means.unsqueeze(1)\n",
    "        ib_x1_std = (1 - channel_importance_1) * channel_std_1.unsqueeze(1)\n",
    "        ib_x1 = ib_x1_mean + torch.rand_like(ib_x1_mean) * ib_x1_std\n",
    "        \n",
    "        epsilon = 1e-8\n",
    "        KL_tensor = 0.5 * (\n",
    "            (ib_x1_std**2) / (channel_std_1.unsqueeze(1) + epsilon)**2 +\n",
    "            (channel_std_1.unsqueeze(1)**2) / (ib_x1_std + epsilon)**2 - 1\n",
    "        ) + (ib_x1_mean**2) / (channel_std_1.unsqueeze(1) + epsilon)**2  # 修改了这里，将 (ib_x_mean - 0)**2 替换为 ib_x_mean**2\n",
    "\n",
    "        KL_Loss_1 = torch.mean(KL_tensor)\n",
    "        \n",
    "        \n",
    "        KL_Loss = KL_Loss_1+0.00001*KL_Loss_2+0.00001*KL_Loss_3\n",
    "        \n",
    "        ib_x1 = ib_x1.permute(0, 2, 1)  # 调整维度顺序\n",
    "        ib_x2 = ib_x2.permute(0, 2, 1)  # 调整维度顺序\n",
    "        ib_x3 = ib_x3.permute(0, 2, 1)  # 调整维度顺序\n",
    "        return ib_x1,ib_x2,ib_x3, KL_Loss, p_1,p_2,p_3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModelWithVAE(nn.Module): \n",
    "    def __init__(self, num_fgs, channel=62, feature_dim=150, hidden_dim=256, latent_dim=64, m_dim=10):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "        - num_fgs: 预测目标的维度\n",
    "        - channel: 每个光谱的通道数（不同频率段）\n",
    "        - feature_dim: 每个光谱的特征维度\n",
    "        - hidden_dim: 隐藏层维度\n",
    "        - latent_dim: 潜在变量 z 的维度\n",
    "        - m_dim: 预测目标的维度（如有需要）\n",
    "        \"\"\"\n",
    "        super(CNNModelWithVAE, self).__init__()\n",
    "        self.channel = channel\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # 创建三个独立的CNN模块\n",
    "        self.cnn1 = IndependentCNN_main(num_fgs)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=62,\n",
    "                                num_heads=2,\n",
    "                                batch_first=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 全连接层用于最终预测，使用 z 和 x3 作为输入\n",
    "        self.fc1 = nn.Linear(channel *feature_dim, 4927)  # z 和 x3\n",
    "        self.fc2 = nn.Linear(4927, 2785)\n",
    "        self.fc3 = nn.Linear(2785, 1574)\n",
    "        self.fc4 = nn.Linear(1574, num_fgs)\n",
    "        self.dropout = nn.Dropout(0.48599073736368)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数：\n",
    "        - x: 输入张量，形状为 [batch_size, 3, feature_dim]\n",
    "        \n",
    "        返回：\n",
    "        - 一个包含预测结果和各类损失组件的字典\n",
    "        \"\"\"\n",
    "        # 拆分输入为三个光谱通道\n",
    "        x1, x2, x3 = x[:, 0:1, :], x[:, 1:2, :], x[:, 2:3, :]  # 每个 [B, 1, feature_dim]\n",
    "\n",
    "        # 分别通过三个独立的CNN\n",
    "        ib_x_1,ib_x_2,ib_x_3, KL_Loss, channal_importance_1,channal_importance_2,channal_importance_3= self.cnn1(x1,x2,x3)\n",
    "    \n",
    "        # 2. 调整输入形状: [B, C, F] -> [F, B, C]\n",
    "        #    假设 feature_dim=F 是序列长度, channel=C 是嵌入维度\n",
    "        ib_x_1_t = ib_x_1.permute(2, 0, 1)  # [F, B, C]\n",
    "        ib_x_2_t = ib_x_2.permute(2, 0, 1)  # [F, B, C]\n",
    "        ib_x_3_t = ib_x_3.permute(2, 0, 1)  # [F, B, C]\n",
    "\n",
    "        # 3. 先以 ib_x_3_t 做 Query，ib_x_1_t 做 Key & Value\n",
    "        out1, _ = self.mha(query=ib_x_3_t, key=ib_x_1_t, value=ib_x_1_t)\n",
    "        # 4. 再以 ib_x_3_t 做 Query，ib_x_2_t 做 Key & Value\n",
    "        out2, _ = self.mha(query=ib_x_3_t, key=ib_x_2_t, value=ib_x_2_t)\n",
    "\n",
    "        # 5. 将两次输出与残差(ib_x_3_t)融合\n",
    "        out = out1 + out2 + ib_x_3_t  # 残差连接，可根据需要再加 LayerNorm 等后处理\n",
    "\n",
    "        # 6. 调整回原形状: [F, B, C] -> [B, C, F]\n",
    "        z = out.permute(1, 2, 0)\n",
    "        # VAE Decoder: 重建三个光谱\n",
    "        x = ib_x_3.view(ib_x_3.size(0), -1)\n",
    "        #现在需要z和x3分开处理，下面先突出x3的处理\n",
    "        z = z.reshape(z.size(0), -1)\n",
    "        ib_x_3= ib_x_3.view(ib_x_3.size(0), -1)\n",
    "        x_pred = F.relu(self.fc1(ib_x_3))  # [B, 4927]\n",
    "        x_pred = self.dropout(x_pred)\n",
    "        x_pred = F.relu(self.fc2(x_pred))  # [B, 2785]\n",
    "        x_pred = self.dropout(x_pred)\n",
    "        x_pred = F.relu(self.fc3(x_pred))  # [B, 1574]\n",
    "        x_pred = self.dropout(x_pred)\n",
    "        x_pred_assit = torch.sigmoid(self.fc4(x_pred))  # [B, num_fgs]\n",
    "        \n",
    "        \n",
    "        x_pred = F.relu(self.fc1(z ))  # [B, 4927]\n",
    "        x_pred = self.dropout(x_pred)\n",
    "        x_pred = F.relu(self.fc2(x_pred))  # [B, 2785]\n",
    "        x_pred = self.dropout(x_pred)\n",
    "        x_pred = F.relu(self.fc3(x_pred))  # [B, 1574]\n",
    "        x_pred = self.dropout(x_pred)\n",
    "        x_pred = torch.sigmoid(self.fc4(x_pred))  # [B, num_fgs]\n",
    "\n",
    "        # KL散度损失取平均值（来自 VAE）\n",
    "        kl=KL_Loss\n",
    "        return {\n",
    "            'x': x_pred,\n",
    "            'ib_x_1': ib_x_1,\n",
    "            'ib_x_2': ib_x_2,\n",
    "            'ib_x_3': ib_x_3,\n",
    "            'kl':kl,\n",
    "            'channal_importance_1':channal_importance_1,\n",
    "            'channal_importance_2':channal_importance_2,\n",
    "            'channal_importance_3':channal_importance_3,\n",
    "            'x_pred_assit':x_pred_assit\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Training function in PyTorch\n",
    "from tqdm import tqdm  # 引入 tqdm\n",
    "\n",
    "b=0.0001\n",
    "# 定义训练函数\n",
    "# 定义训练函数\n",
    "from tqdm import tqdm  # 引入 tqdm\n",
    "\n",
    "# 定义训练函数\n",
    "def train_model(X_train, y_train, X_test, y_test, num_fgs, weighted=False, batch_size=41, epochs=41, \n",
    "                annealing_epochs=10, max_lambda_kl=1.0, lambda_cmi=0.5, lambda_recon=0.0001):\n",
    "    device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CNNModelWithVAE(num_fgs).to(device)\n",
    "    \n",
    "    # 定义优化器和损失函数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    if weighted:\n",
    "        class_weights = calculate_class_weights(y_train)\n",
    "        criterion = WeightedBinaryCrossEntropyLoss(class_weights).to(device)\n",
    "    else:\n",
    "        criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    # 创建 DataLoader\n",
    "    y_train = np.array([np.array(item, dtype=np.float32) for item in y_train], dtype=np.float32)\n",
    "    y_test = np.array([np.array(item, dtype=np.float32) for item in y_test], dtype=np.float32)\n",
    "    train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 确保保存路径存在\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    best_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        recon_loss_avg = 0.0\n",
    "        kl_weight = min(max_lambda_kl, (epoch + 1) / annealing_epochs)\n",
    "        with tqdm(train_loader, unit='batch', desc=f\"Epoch {epoch+1}/{epochs}\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                inputs, targets = batch\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                x_pred = outputs['x']\n",
    "                kl = outputs['kl']\n",
    "                x_pred_assit = outputs['x_pred_assit']\n",
    "\n",
    "                # 预测损失\n",
    "                assit_loss = criterion(x_pred_assit, targets)\n",
    "                pred_loss = criterion(x_pred, targets)\n",
    "                \n",
    "        \n",
    "\n",
    "                # 总损失：预测损失 + KL散度 + 互信息损失 + 重建损失\n",
    "                total_loss =pred_loss +   + 0.000000000001 * kl+assit_loss\n",
    "                total_loss.backward()\n",
    "                \n",
    "                # 梯度裁剪\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += total_loss.item()\n",
    "                tepoch.set_postfix(loss=running_loss / (tepoch.n + 1),\n",
    "                                  kl_weight=kl_weight)\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss}, KL Weight: {kl_weight}')\n",
    "        \n",
    "        # 评估F1分数\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                inputs, targets = batch\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                x_pred = outputs['x']\n",
    "                predictions.append(x_pred.cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        binary_predictions = (predictions > 0.5).astype(int)\n",
    "        f1 = f1_score(y_test, binary_predictions, average='micro')\n",
    "        print(f'F1 Score: {f1}')\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            model_save_path = out_path / \"best_model.pth\"\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f'Best model saved with F1 Score: {best_f1} at {model_save_path}')\n",
    "\n",
    "    return binary_predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Custom loss function with class weights\n",
    "class WeightedBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedBinaryCrossEntropyLoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = self.class_weights[0] * (1 - y_true) * torch.log(1 - y_pred + 1e-15) + \\\n",
    "               self.class_weights[1] * y_true * torch.log(y_pred + 1e-15)\n",
    "        return -loss.mean()\n",
    "\n",
    "# Calculate class weights\n",
    "def calculate_class_weights(y_true):\n",
    "    num_samples = y_true.shape[0]\n",
    "    class_weights = np.zeros((2, y_true.shape[1]))\n",
    "    for i in range(y_true.shape[1]):\n",
    "        weights_n = num_samples / (2 * (y_true[:, i] == 0).sum())\n",
    "        weights_p = num_samples / (2 * (y_true[:, i] == 1).sum())\n",
    "        class_weights[0, i] = weights_n\n",
    "        class_weights[1, i] = weights_p\n",
    "    return torch.tensor(class_weights.T, dtype=torch.float32)\n",
    "\n",
    "# Loading data (no change)\n",
    "analytical_data = Path(\"/data/zjh2/multimodal-spectroscopic-dataset-main/data/multimodal_spectroscopic_dataset\")\n",
    "out_path = Path(\"/home/dwj/icml_guangpu/multimodal-spectroscopic-dataset-main/runs/runs_f_groups/all\")\n",
    "columns = [\"h_nmr_spectra\", \"c_nmr_spectra\", \"ir_spectra\"]\n",
    "seed = 3245\n",
    "\n",
    "# 准备存储合并后的数据\n",
    "all_data = []\n",
    "i=0\n",
    "# 一次性读取文件并处理所有列\n",
    "for parquet_file in analytical_data.glob(\"*.parquet\"):\n",
    "    i+=1\n",
    "    # 读取所有需要的列\n",
    "    data = pd.read_parquet(parquet_file, columns=columns + ['smiles'])\n",
    "    \n",
    "    # 对每个列进行插值\n",
    "    for column in columns:\n",
    "        data[column] = data[column].map(interpolate_to_600)\n",
    "    \n",
    "    # 添加功能团信息\n",
    "    data['func_group'] = data.smiles.map(get_functional_groups)\n",
    "    #在这里就是0/1矩阵了\n",
    "    all_data.append(data)\n",
    "    print(f\"Loaded Data from: \", i)\n",
    "    if i == 3:\n",
    "        break\n",
    "# 合并所有数据\n",
    "training_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "\n",
    "# 将数据划分为训练集和测试集\n",
    "train, test = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "\n",
    "# 定义特征列\n",
    "columns = [\"h_nmr_spectra\", \"c_nmr_spectra\", \"ir_spectra\"]\n",
    "\n",
    "# 提取训练集特征和标签\n",
    "X_train = np.array(train[columns].values.tolist())  # 确保特征值是一个二维数组\n",
    "y_train = np.array(train['func_group'].values)      # 标签转换为一维数组\n",
    "\n",
    "# 提取测试集特征和标签\n",
    "X_test = np.array(test[columns].values.tolist())    # 同样确保二维数组\n",
    "y_test = np.array(test['func_group'].values)        # 标签一维数组\n",
    "\n",
    "# 检查数组形状以验证正确性\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "# Train extended model\n",
    "predictions = train_model(X_train, y_train, X_test, y_test,num_fgs=37, weighted=False, batch_size=41, epochs=41, \n",
    "                annealing_epochs=10, max_lambda_kl=1.0, lambda_cmi=0.1, lambda_recon=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_test = np.array([np.array(item, dtype=np.float32) for item in y_test], dtype=np.float32)\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Save results\n",
    "with open(out_path / \"results.pickle\", \"wb\") as file:\n",
    "    pickle.dump({'pred': predictions, 'tgt': y_test}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (159777578.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    0.6583553708600932    0.000001 时\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 0.7599297012302285  0.0时\n",
    "\n",
    "0.6583553708600932    0.000001 时\n",
    "0.7347661785545584        0.000000001 \n",
    " 0.7590207680705419     0.6824257425742575（如果只用z预测的haul必须附带损失。）\n",
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
