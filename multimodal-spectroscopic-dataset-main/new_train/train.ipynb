{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Disable RDLogger warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import os\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "functional_groups = {\n",
    "    'Acid anhydride': Chem.MolFromSmarts('[CX3](=[OX1])[OX2][CX3](=[OX1])'),\n",
    "    'Acyl halide': Chem.MolFromSmarts('[CX3](=[OX1])[F,Cl,Br,I]'),\n",
    "    'Alcohol': Chem.MolFromSmarts('[#6][OX2H]'),\n",
    "    'Aldehyde': Chem.MolFromSmarts('[CX3H1](=O)[#6,H]'),\n",
    "    'Alkane': Chem.MolFromSmarts('[CX4;H3,H2]'),\n",
    "    'Alkene': Chem.MolFromSmarts('[CX3]=[CX3]'),\n",
    "    'Alkyne': Chem.MolFromSmarts('[CX2]#[CX2]'),\n",
    "    'Amide': Chem.MolFromSmarts('[NX3][CX3](=[OX1])[#6]'),\n",
    "    'Amine': Chem.MolFromSmarts('[NX3;H2,H1,H0;!$(NC=O)]'),\n",
    "    'Arene': Chem.MolFromSmarts('[cX3]1[cX3][cX3][cX3][cX3][cX3]1'),\n",
    "    'Azo compound': Chem.MolFromSmarts('[#6][NX2]=[NX2][#6]'),\n",
    "    'Carbamate': Chem.MolFromSmarts('[NX3][CX3](=[OX1])[OX2H0]'),\n",
    "    'Carboxylic acid': Chem.MolFromSmarts('[CX3](=O)[OX2H]'),\n",
    "    'Enamine': Chem.MolFromSmarts('[NX3][CX3]=[CX3]'),\n",
    "    'Enol': Chem.MolFromSmarts('[OX2H][#6X3]=[#6]'),\n",
    "    'Ester': Chem.MolFromSmarts('[#6][CX3](=O)[OX2H0][#6]'),\n",
    "    'Ether': Chem.MolFromSmarts('[OD2]([#6])[#6]'),\n",
    "    'Haloalkane': Chem.MolFromSmarts('[#6][F,Cl,Br,I]'),\n",
    "    'Hydrazine': Chem.MolFromSmarts('[NX3][NX3]'),\n",
    "    'Hydrazone': Chem.MolFromSmarts('[NX3][NX2]=[#6]'),\n",
    "    'Imide': Chem.MolFromSmarts('[CX3](=[OX1])[NX3][CX3](=[OX1])'),\n",
    "    'Imine': Chem.MolFromSmarts('[$([CX3]([#6])[#6]),$([CX3H][#6])]=[$([NX2][#6]),$([NX2H])]'),\n",
    "    'Isocyanate': Chem.MolFromSmarts('[NX2]=[C]=[O]'),\n",
    "    'Isothiocyanate': Chem.MolFromSmarts('[NX2]=[C]=[S]'),\n",
    "    'Ketone': Chem.MolFromSmarts('[#6][CX3](=O)[#6]'),\n",
    "    'Nitrile': Chem.MolFromSmarts('[NX1]#[CX2]'),\n",
    "    'Phenol': Chem.MolFromSmarts('[OX2H][cX3]:[c]'),\n",
    "    'Phosphine': Chem.MolFromSmarts('[PX3]'),\n",
    "    'Sulfide': Chem.MolFromSmarts('[#16X2H0]'),\n",
    "    'Sulfonamide': Chem.MolFromSmarts('[#16X4]([NX3])(=[OX1])(=[OX1])[#6]'),\n",
    "    'Sulfonate': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[OX2H0]'),\n",
    "    'Sulfone': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[#6]'),\n",
    "    'Sulfonic acid': Chem.MolFromSmarts('[#16X4](=[OX1])(=[OX1])([#6])[OX2H]'),\n",
    "    'Sulfoxide': Chem.MolFromSmarts('[#16X3]=[OX1]'),\n",
    "    'Thial': Chem.MolFromSmarts('[CX3H1](=S)[#6,H]'),\n",
    "    'Thioamide': Chem.MolFromSmarts('[NX3][CX3]=[SX1]'),\n",
    "    'Thiol': Chem.MolFromSmarts('[#16X2H]')\n",
    "}\n",
    "def match_group(mol: Chem.Mol, func_group) -> int:\n",
    "    if type(func_group) == Chem.Mol:\n",
    "        n = len(mol.GetSubstructMatches(func_group))\n",
    "    else:\n",
    "        n = func_group(mol)\n",
    "    return 0 if n == 0 else 1\n",
    "# Function to map SMILES to functional groups (no change)\n",
    "def get_functional_groups(smiles: str) -> dict:\n",
    "    smiles = smiles.strip().replace(' ', '')\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: \n",
    "        return None\n",
    "    func_groups = [match_group(mol, smarts) for smarts in functional_groups.values()]\n",
    "    return func_groups\n",
    "\n",
    "def interpolate_to_600(spec):\n",
    "    old_x = np.arange(len(spec))\n",
    "    new_x = np.linspace(min(old_x), max(old_x), 600)\n",
    "    interp = interp1d(old_x, spec)\n",
    "    return interp(new_x)\n",
    "\n",
    "def make_msms_spectrum(spectrum):\n",
    "    msms_spectrum = np.zeros(10000)\n",
    "    for peak in spectrum:\n",
    "        peak_pos = int(peak[0]*10)\n",
    "        peak_pos = min(peak_pos, 9999)\n",
    "        msms_spectrum[peak_pos] = peak[1]\n",
    "    return msms_spectrum\n",
    "\n",
    "# Define CNN Model in PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IndependentCNN(nn.Module):\n",
    "    def __init__(self, num_fgs):\n",
    "        super(IndependentCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=31, kernel_size=11, padding='same')\n",
    "        self.conv2 = nn.Conv1d(in_channels=31, out_channels=62, kernel_size=11, padding='same')\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(31)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(62)\n",
    "\n",
    "        # MLP for selecting important channels (62 channels)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(150, 128),  # Input 150 features per channel\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)     # Output importance score for each channel\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batch_norm1(self.conv1(x)))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = F.relu(self.batch_norm2(self.conv2(x)))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "\n",
    "        # 通道重要性计算\n",
    "        static_feature_map = x.clone().detach()\n",
    "        channel_means = x.mean(dim=1)\n",
    "        channel_std = x.std(dim=1)\n",
    "\n",
    "        channel_importance = torch.sigmoid(self.mlp(x))\n",
    "        ib_x_mean = x * channel_importance + (1 - channel_importance) * channel_means.unsqueeze(1)\n",
    "        ib_x_std = (1 - channel_importance) * channel_std.unsqueeze(1)\n",
    "        ib_x = ib_x_mean + torch.rand_like(ib_x_mean) * ib_x_std\n",
    "\n",
    "        # KL Divergence loss\n",
    "        epsilon = 1e-8\n",
    "        KL_tensor = 0.5 * (\n",
    "            (ib_x_std**2) / (channel_std.unsqueeze(1) + epsilon)**2 +\n",
    "            (channel_std.unsqueeze(1)**2) / (ib_x_std + epsilon)**2 - 1\n",
    "        ) + ((ib_x_mean - channel_means.unsqueeze(1))**2) / (channel_std.unsqueeze(1) + epsilon)**2\n",
    "\n",
    "        KL_Loss = torch.mean(KL_tensor)\n",
    "\n",
    "        # Flatten and pass through fully connected layers\n",
    "\n",
    "        return ib_x, KL_Loss\n",
    "\n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_dim]\n",
    "        q = self.query(x)  # [batch_size, seq_len, input_dim]\n",
    "        k = self.key(x)    # [batch_size, seq_len, input_dim]\n",
    "        v = self.value(x)  # [batch_size, seq_len, input_dim]\n",
    "        # 计算注意力分数\n",
    "        attention_scores = torch.bmm(q, k.transpose(1, 2))  # [batch_size, seq_len, seq_len]\n",
    "        attention_probs = self.softmax(attention_scores)   # [batch_size, seq_len, seq_len]\n",
    "        # 加权求和\n",
    "        output = torch.bmm(attention_probs, v)  # [batch_size, seq_len, input_dim]\n",
    "        return output\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module): \n",
    "    def __init__(self, num_fgs):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # 创建三个独立的CNN模块\n",
    "        self.cnn1 = IndependentCNN(num_fgs)\n",
    "        self.cnn2 = IndependentCNN(num_fgs)\n",
    "        self.cnn3 = IndependentCNN(num_fgs)\n",
    "\n",
    "        # 自注意力层，用于信息交互\n",
    "        self.attention = SelfAttentionLayer(input_dim=150)\n",
    "\n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(62 * 150, 4927)\n",
    "        self.fc2 = nn.Linear(4927, 2785)\n",
    "        self.fc3 = nn.Linear(2785, 1574)\n",
    "        self.fc4 = nn.Linear(1574, num_fgs)\n",
    "        self.dropout = nn.Dropout(0.48599073736368)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 拆分输入为三个通道\n",
    "        x1, x2, x3 = x[:, 0:1, :], x[:, 1:2, :], x[:, 2:3, :]\n",
    "\n",
    "        # 分别通过三个独立的CNN\n",
    "        ib_x_1, kl1 = self.cnn1(x1)\n",
    "        ib_x_2, kl2 = self.cnn2(x2)\n",
    "        ib_x_3, kl3 = self.cnn3(x3)\n",
    "\n",
    "        # 整合三个通道的输出\n",
    "        # 将三个通道堆叠后通过自注意力机制增强交互\n",
    "        # 继续进行预测\n",
    "        x = ib_x_3.view(ib_x_3.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "\n",
    "        # KL损失取平均值\n",
    "        kl_loss = (kl1 + kl2 + kl3) / 3\n",
    "        return x, kl_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function in PyTorch\n",
    "from tqdm import tqdm  # 引入 tqdm\n",
    "\n",
    "b=0.0\n",
    "def train_model(X_train, y_train, X_test,y_test, num_fgs, weighted=False, batch_size=41, epochs=41):\n",
    "    device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CNNModel(num_fgs).to(device)\n",
    "    \n",
    "    # Define optimizer and loss\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    if weighted:\n",
    "        class_weights = calculate_class_weights(y_train)\n",
    "        criterion = WeightedBinaryCrossEntropyLoss(class_weights).to(device)\n",
    "    else:\n",
    "        criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    # Create DataLoader\n",
    "    y_train = np.array([np.array(item, dtype=np.float32) for item in y_train], dtype=np.float32)\n",
    "    y_test = np.array([np.array(item, dtype=np.float32) for item in y_test], dtype=np.float32)\n",
    "    train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Create tqdm progress bar for each epoch\n",
    "        with tqdm(train_loader, unit='batch', desc=f\"Epoch {epoch+1}/{epochs}\") as tepoch:\n",
    "            for inputs, targets in tepoch:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs,loss1 = model(inputs)  # Add channel dimension\n",
    "                loss2 = criterion(outputs, targets)\n",
    "                loss = loss2+loss1*b\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Update the progress bar with loss information\n",
    "                tepoch.set_postfix(loss=running_loss / (tepoch.n + 1))\n",
    "        \n",
    "        # After every epoch, print the average loss\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs,loss2 = model(inputs)\n",
    "                predictions.append(outputs.cpu().numpy())\n",
    "                \n",
    "\n",
    "        predictions = np.concatenate(predictions)\n",
    "        predictions = (predictions > 0.5).astype(int)\n",
    "        y_test = np.array([np.array(item, dtype=np.float32) for item in y_test], dtype=np.float32)\n",
    "        f1 = f1_score(y_test, predictions, average='micro')\n",
    "        print(f'F1 Score: {f1}')\n",
    "\n",
    "        \n",
    "    return (predictions > 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Custom loss function with class weights\n",
    "class WeightedBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedBinaryCrossEntropyLoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = self.class_weights[0] * (1 - y_true) * torch.log(1 - y_pred + 1e-15) + \\\n",
    "               self.class_weights[1] * y_true * torch.log(y_pred + 1e-15)\n",
    "        return -loss.mean()\n",
    "\n",
    "# Calculate class weights\n",
    "def calculate_class_weights(y_true):\n",
    "    num_samples = y_true.shape[0]\n",
    "    class_weights = np.zeros((2, y_true.shape[1]))\n",
    "    for i in range(y_true.shape[1]):\n",
    "        weights_n = num_samples / (2 * (y_true[:, i] == 0).sum())\n",
    "        weights_p = num_samples / (2 * (y_true[:, i] == 1).sum())\n",
    "        class_weights[0, i] = weights_n\n",
    "        class_weights[1, i] = weights_p\n",
    "    return torch.tensor(class_weights.T, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [[144.01247, C#CN1C=CC=[SH]1([O-])O], [156.012...\n",
      "1       [[180.95058, COC(=O)C(Br)C[O-]], [204.98697, C...\n",
      "2       [[169.07712, [C-]#CN=C1C=CN=C2C=CCCC12], [177....\n",
      "3       [[101.01452, [CH-]=C=C=C=C=NC#N], [110.04115, ...\n",
      "4       [[120.01247, C#S(O)(O)CC=[N-]], [121.03287, C#...\n",
      "                              ...                        \n",
      "3197    [[104.05057, [C-]1=CCN2C=CC=C12], [106.06622, ...\n",
      "3198    [[101.02083, [CH-]=C(F)C#CCF], [138.01608, N#C...\n",
      "3199    [[108.02034, [N-]=CC=C=NN=C=O], [110.03599, C=...\n",
      "3200    [[116.05057, [CH-]=C1C=C(C#N)C=CC1], [164.0717...\n",
      "3201    [[107.05024, C#CCC(C#C)O[CH2-]], [121.06589, C...\n",
      "Name: msms_fragments_negative, Length: 3202, dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data[column])\n\u001b[0;32m---> 19\u001b[0m     data[column] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterpolate_to_600\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 添加功能团信息\u001b[39;00m\n\u001b[1;32m     22\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunc_group\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msmiles\u001b[38;5;241m.\u001b[39mmap(get_functional_groups)\n",
      "File \u001b[0;32m~/anaconda3/envs/zs/lib/python3.9/site-packages/pandas/core/series.py:4397\u001b[0m, in \u001b[0;36mSeries.map\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\n\u001b[1;32m   4319\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4320\u001b[0m     arg: Callable \u001b[38;5;241m|\u001b[39m Mapping \u001b[38;5;241m|\u001b[39m Series,\n\u001b[1;32m   4321\u001b[0m     na_action: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4322\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4324\u001b[0m \u001b[38;5;124;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[1;32m   4325\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4395\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[1;32m   4396\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4397\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   4399\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4400\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/zs/lib/python3.9/site-packages/pandas/core/base.py:924\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# mapper is a function\u001b[39;00m\n\u001b[0;32m--> 924\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mmap_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_values\n",
      "File \u001b[0;32m~/anaconda3/envs/zs/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[2], line 78\u001b[0m, in \u001b[0;36minterpolate_to_600\u001b[0;34m(spec)\u001b[0m\n\u001b[1;32m     76\u001b[0m old_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(spec))\n\u001b[1;32m     77\u001b[0m new_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;28mmin\u001b[39m(old_x), \u001b[38;5;28mmax\u001b[39m(old_x), \u001b[38;5;241m600\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m interp \u001b[38;5;241m=\u001b[39m \u001b[43minterp1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m interp(new_x)\n",
      "File \u001b[0;32m~/anaconda3/envs/zs/lib/python3.9/site-packages/scipy/interpolate/_interpolate.py:522\u001b[0m, in \u001b[0;36minterp1d.__init__\u001b[0;34m(self, x, y, kind, axis, copy, bounds_error, fill_value, assume_sorted)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# Force-cast y to a floating-point type, if it's not yet one\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(y\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39minexact):\n\u001b[0;32m--> 522\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# Backward compatibility\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m axis \u001b[38;5;241m%\u001b[39m y\u001b[38;5;241m.\u001b[39mndim\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Loading data (no change)\n",
    "analytical_data = Path(\"/data/zjh2/multimodal-spectroscopic-dataset-main/data/multimodal_spectroscopic_dataset\")\n",
    "out_path = Path(\"/home/dwj/icml_guangpu/multimodal-spectroscopic-dataset-main/runs/runs_f_groups/all\")\n",
    "columns = [\"msms_fragments_negative\", \"msms_fragments_negative\", \"ir_spectra\"]\n",
    "seed = 3245\n",
    "\n",
    "# 准备存储合并后的数据\n",
    "all_data = []\n",
    "i=0\n",
    "# 一次性读取文件并处理所有列\n",
    "for parquet_file in analytical_data.glob(\"*.parquet\"):\n",
    "    i+=1\n",
    "    # 读取所有需要的列\n",
    "    #['smiles', 'hsqc_nmr_peaks', 'hsqc_nmr_spectrum', 'h_nmr_peaks', 'h_nmr_spectra', 'molecular_formula', 'c_nmr_peaks', 'ir_spectra', 'msms_positive_10ev', 'msms_positive_20ev', 'msms_positive_40ev', 'msms_fragments_positive', 'msms_negative_10ev', 'msms_negative_20ev', 'msms_negative_40ev', 'msms_fragments_negative', 'c_nmr_spectra']\n",
    "    \n",
    "    # 对每个列进行插值\n",
    "    for column in columns:\n",
    "        print(data[column])\n",
    "        data[column] = data[column].map(interpolate_to_600)\n",
    "    \n",
    "    # 添加功能团信息\n",
    "    data['func_group'] = data.smiles.map(get_functional_groups)\n",
    "    all_data.append(data)\n",
    "    if i==3:\n",
    "        break\n",
    "    print(f\"Loaded Data from: \", i)\n",
    "    \n",
    "# 合并所有数据\n",
    "training_data = pd.concat(all_data, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8740, 3, 600)\n",
      "y_train shape: (8740,)\n",
      "X_test shape: (972, 3, 600)\n",
      "y_test shape: (972,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 将数据划分为训练集和测试集\n",
    "train, test = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "\n",
    "# 定义特征列\n",
    "columns = [\"h_nmr_spectra\", \"c_nmr_spectra\", \"ir_spectra\"]\n",
    "\n",
    "# 提取训练集特征和标签\n",
    "X_train = np.array(train[columns].values.tolist())  # 确保特征值是一个二维数组\n",
    "y_train = np.array(train['func_group'].values)      # 标签转换为一维数组\n",
    "\n",
    "# 提取测试集特征和标签\n",
    "X_test = np.array(test[columns].values.tolist())    # 同样确保二维数组\n",
    "y_test = np.array(test['func_group'].values)        # 标签一维数组\n",
    "\n",
    "# 检查数组形状以验证正确性\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/41: 100%|██████████| 214/214 [00:03<00:00, 63.49batch/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/41, Loss: 0.19838749847122442\n",
      "F1 Score: 0.7249100823761457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/41: 100%|██████████| 214/214 [00:02<00:00, 75.47batch/s, loss=0.173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/41, Loss: 0.16921533101072936\n",
      "F1 Score: 0.693950633073283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/41: 100%|██████████| 214/214 [00:02<00:00, 76.02batch/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/41, Loss: 0.1615283143158271\n",
      "F1 Score: 0.746236036911122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/41: 100%|██████████| 214/214 [00:02<00:00, 75.59batch/s, loss=0.159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/41, Loss: 0.1550841910120483\n",
      "F1 Score: 0.7512972125015084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/41: 100%|██████████| 214/214 [00:02<00:00, 75.55batch/s, loss=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/41, Loss: 0.1487691719607215\n",
      "F1 Score: 0.760258127359065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/41: 100%|██████████| 214/214 [00:02<00:00, 75.64batch/s, loss=0.147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/41, Loss: 0.1432664247804156\n",
      "F1 Score: 0.7644944896981313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/41: 100%|██████████| 214/214 [00:02<00:00, 75.22batch/s, loss=0.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/41, Loss: 0.13819755272609052\n",
      "F1 Score: 0.7744782349433512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/41: 100%|██████████| 214/214 [00:02<00:00, 76.09batch/s, loss=0.137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/41, Loss: 0.13347966965531635\n",
      "F1 Score: 0.7772130953787968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/41: 100%|██████████| 214/214 [00:02<00:00, 74.99batch/s, loss=0.132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/41, Loss: 0.129216957872159\n",
      "F1 Score: 0.7768654052124241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/41: 100%|██████████| 214/214 [00:03<00:00, 70.81batch/s, loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/41, Loss: 0.12426332523611104\n",
      "F1 Score: 0.7838435572110348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/41: 100%|██████████| 214/214 [00:02<00:00, 71.94batch/s, loss=0.124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/41, Loss: 0.12051574523760894\n",
      "F1 Score: 0.779483517791265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/41: 100%|██████████| 214/214 [00:02<00:00, 72.12batch/s, loss=0.117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/41, Loss: 0.11664496191611914\n",
      "F1 Score: 0.7827614474762853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/41: 100%|██████████| 214/214 [00:02<00:00, 75.31batch/s, loss=0.116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/41, Loss: 0.11298175020334876\n",
      "F1 Score: 0.7826909434854575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/41: 100%|██████████| 214/214 [00:02<00:00, 75.24batch/s, loss=0.113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/41, Loss: 0.11023581226434663\n",
      "F1 Score: 0.7910755148741418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/41: 100%|██████████| 214/214 [00:02<00:00, 75.62batch/s, loss=0.108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/41, Loss: 0.10509562224288967\n",
      "F1 Score: 0.7891377509574098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/41: 100%|██████████| 214/214 [00:02<00:00, 73.41batch/s, loss=0.105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/41, Loss: 0.10226536006013924\n",
      "F1 Score: 0.7866002086472702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/41: 100%|██████████| 214/214 [00:02<00:00, 74.05batch/s, loss=0.101] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/41, Loss: 0.09839240398919472\n",
      "F1 Score: 0.793810888252149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/41: 100%|██████████| 214/214 [00:02<00:00, 74.36batch/s, loss=0.0979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/41, Loss: 0.09558368898997797\n",
      "F1 Score: 0.7902235607552415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/41: 100%|██████████| 214/214 [00:02<00:00, 75.45batch/s, loss=0.0948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/41, Loss: 0.0925976395711442\n",
      "F1 Score: 0.7880971754039289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/41: 100%|██████████| 214/214 [00:03<00:00, 70.71batch/s, loss=0.0889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/41, Loss: 0.08846271797039798\n",
      "F1 Score: 0.7933941563690957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/41: 100%|██████████| 214/214 [00:02<00:00, 74.76batch/s, loss=0.0873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/41, Loss: 0.08527897056366239\n",
      "F1 Score: 0.789249048333141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/41: 100%|██████████| 214/214 [00:02<00:00, 71.91batch/s, loss=0.0866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/41, Loss: 0.08453464725655373\n",
      "F1 Score: 0.7889070910367423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/41: 100%|██████████| 214/214 [00:02<00:00, 73.93batch/s, loss=0.0835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/41, Loss: 0.08119736899073436\n",
      "F1 Score: 0.7925091911764706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/41: 100%|██████████| 214/214 [00:02<00:00, 73.28batch/s, loss=0.082] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/41, Loss: 0.08010686540575786\n",
      "F1 Score: 0.788520669627605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/41: 100%|██████████| 214/214 [00:02<00:00, 75.77batch/s, loss=0.0788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/41, Loss: 0.07696748397826592\n",
      "F1 Score: 0.7874678512976385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/41: 100%|██████████| 214/214 [00:02<00:00, 76.13batch/s, loss=0.0758]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/41, Loss: 0.0739802256679145\n",
      "F1 Score: 0.7928603910868577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/41: 100%|██████████| 214/214 [00:02<00:00, 72.35batch/s, loss=0.074] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/41, Loss: 0.07190109813742548\n",
      "F1 Score: 0.7882093536033422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/41: 100%|██████████| 214/214 [00:02<00:00, 72.81batch/s, loss=0.0718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/41, Loss: 0.07010885961701936\n",
      "F1 Score: 0.7881605806305285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/41: 100%|██████████| 214/214 [00:02<00:00, 74.01batch/s, loss=0.0697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/41, Loss: 0.06808580108265454\n",
      "F1 Score: 0.7846800560485754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/41: 100%|██████████| 214/214 [00:03<00:00, 66.69batch/s, loss=0.0676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/41, Loss: 0.06630532011807522\n",
      "F1 Score: 0.7889528193325661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/41: 100%|██████████| 214/214 [00:03<00:00, 66.43batch/s, loss=0.0639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/41, Loss: 0.06389833725640708\n",
      "F1 Score: 0.7865530739120423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/41: 100%|██████████| 214/214 [00:02<00:00, 71.79batch/s, loss=0.0629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/41, Loss: 0.0626512288733899\n",
      "F1 Score: 0.7827394724664507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/41: 100%|██████████| 214/214 [00:02<00:00, 74.91batch/s, loss=0.0619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/41, Loss: 0.060419339199211\n",
      "F1 Score: 0.7896056530658765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/41: 100%|██████████| 214/214 [00:02<00:00, 73.71batch/s, loss=0.06]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/41, Loss: 0.058606228904328615\n",
      "F1 Score: 0.7835507921714818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/41: 100%|██████████| 214/214 [00:02<00:00, 72.86batch/s, loss=0.0594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/41, Loss: 0.057970114191558876\n",
      "F1 Score: 0.7912914624415821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/41: 100%|██████████| 214/214 [00:02<00:00, 75.09batch/s, loss=0.0581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/41, Loss: 0.056758484246087405\n",
      "F1 Score: 0.788738841840238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/41: 100%|██████████| 214/214 [00:02<00:00, 74.37batch/s, loss=0.0573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/41, Loss: 0.055943220000818514\n",
      "F1 Score: 0.7864727608494921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/41:  59%|█████▉    | 127/214 [00:01<00:01, 75.93batch/s, loss=0.0547]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train extended model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_train[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_fgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m37\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweighted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39marray(item, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m y_test], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(X_train, y_train, X_test, y_test, num_fgs, weighted, batch_size, epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m loss2 \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss2\u001b[38;5;241m+\u001b[39mloss1\u001b[38;5;241m*\u001b[39mb\n\u001b[0;32m---> 40\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     43\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/zs/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zs/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train extended model\n",
    "print(y_train[0])\n",
    "predictions = train_model(X_train, y_train, X_test, y_test,num_fgs=37, weighted=False)\n",
    "\n",
    "# Evaluate the model\n",
    "y_test = np.array([np.array(item, dtype=np.float32) for item in y_test], dtype=np.float32)\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Save results\n",
    "with open(out_path / \"results.pickle\", \"wb\") as file:\n",
    "    pickle.dump({'pred': predictions, 'tgt': y_test}, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
